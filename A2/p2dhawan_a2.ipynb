{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOU_a2 (v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q1: Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer here (double-click)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To help you with $\\LaTeX$, and to show you my expectations, here is a sample taken from the lecture notes, taken from the 3rd and 4th page of the notes entitled \"Error Backpropagation\". It has nothing to do with the solution to this question, but just demonstrates some of the features of $\\LaTeX$. Notice how I include English statments to guide the reader through the derivation.\n",
    "\n",
    "<a target=_new href=\"http://detexify.kirelabs.org/classify.html\">This web page</a> is very handy for identifying $\\LaTeX$ symbols.\n",
    "\n",
    "---\n",
    "More generally, for $\\vec{x} \\in \\mathbb{R}^X$, $\\vec{h} \\in \\mathbb{R}^H$, and $\\vec{y} \\in \\mathbb{R}^Y$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\alpha_i}\n",
    "  &= \\frac{d h_i}{d \\alpha_i} \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "  \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right] \\cdot\n",
    "  \\left[ \\frac{\\partial E}{\\partial \\beta_1} \\ \\cdots \\ \\frac{\\partial E}{\\partial \\beta_Y} \\right] \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "   \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right]\n",
    "   \\left[ \\begin{array}{c}\n",
    "     \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\frac{\\partial E}{\\partial \\beta_Y} \\end{array} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, for all elements,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\alpha_H}\n",
    "\\end{array} \\right] &=\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{d h_1}{d \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{d h_H}{d \\alpha_H}\n",
    "\\end{array} \\right]\n",
    "\\odot\n",
    "\\left[ \\begin{array}{ccc}\n",
    "  M_{11} & \\cdots & M_{Y1} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  M_{1H} & \\cdots & M_{YH}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\beta_Y}\n",
    "\\end{array} \\right] \\\\\n",
    "%\n",
    "\\frac{\\partial E}{\\partial \\vec{\\alpha}} &=\n",
    "\\frac{d \\vec{h}}{d \\vec{\\alpha}} \\odot M^\\mathrm{T}\n",
    "\\frac{\\partial E}{\\partial \\vec{\\beta}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q3: Top-Layer Error Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q4: Implementing Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Supplied Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     16,
     35
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Supplied functions\n",
    "\n",
    "def NSamples(x):\n",
    "    '''\n",
    "        n = NSamples(x)\n",
    "        \n",
    "        Returns the number of samples in a batch of inputs.\n",
    "        \n",
    "        Input:\n",
    "         x   is a 2D array\n",
    "        \n",
    "        Output:\n",
    "         n   is an integer\n",
    "    '''\n",
    "    return len(x)\n",
    "\n",
    "def Shuffle(inputs, targets):\n",
    "    '''\n",
    "        s_inputs, s_targets = Shuffle(inputs, targets)\n",
    "        \n",
    "        Randomly shuffles the dataset.\n",
    "        \n",
    "        Inputs:\n",
    "         inputs     array of inputs\n",
    "         targets    array of corresponding targets\n",
    "         \n",
    "        Outputs:\n",
    "         s_inputs   shuffled array of inputs\n",
    "         s_targets  corresponding shuffled array of targets\n",
    "    '''\n",
    "    data = list(zip(inputs,targets))\n",
    "    np.random.shuffle(data)\n",
    "    s_inputs, s_targets = zip(*data)\n",
    "    return np.array(s_inputs), np.array(s_targets)\n",
    "\n",
    "def Logistic(z):\n",
    "    '''\n",
    "        y = Logistic(z)\n",
    "\n",
    "        Applies the logistic function to each element in z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return 1. / (1 + np.exp(-z) )\n",
    "\n",
    "def Logistic_p(h):\n",
    "    '''\n",
    "        yp = Logistic_p(h)\n",
    "        \n",
    "        Returns the slope of the logistic function at z when h = Logistic(z).\n",
    "        Note the h is the input, NOT z.\n",
    "    '''\n",
    "    return h*(1.-h)\n",
    "\n",
    "def Identity(z):\n",
    "    '''\n",
    "        y = Identity(z)\n",
    "\n",
    "        Does nothing... simply returns z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return z\n",
    "\n",
    "def Identity_p(h):\n",
    "    '''\n",
    "        yp = Identity_p(h)\n",
    "        \n",
    "        Returns the slope of the identity function h.\n",
    "    '''\n",
    "    return np.ones_like(h)\n",
    "\n",
    "def OneHot(z):\n",
    "    '''\n",
    "        y = OneHot(z)\n",
    "\n",
    "        Applies the one-hot function to the vectors in z.\n",
    "        Example:\n",
    "          OneHot([[0.9, 0.1], [-0.5, 0.1]])\n",
    "          returns np.array([[1,0],[0,1]])\n",
    "\n",
    "        Input:\n",
    "         z    is a 2D array of samples\n",
    "\n",
    "        Output:\n",
    "         y    is an array the same shape as z\n",
    "    '''\n",
    "    y = []\n",
    "    # Locate the max of each row\n",
    "    for zz in z:\n",
    "        idx = np.argmax(zz)\n",
    "        b = np.zeros_like(zz)\n",
    "        b[idx] = 1.\n",
    "        y.append(b)\n",
    "    y = np.array(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def __init__(self, n_nodes, act='logistic'):\n",
    "        '''\n",
    "            lyr = Layer(n_nodes, act='logistic')\n",
    "            \n",
    "            Creates a layer object.\n",
    "            \n",
    "            Inputs:\n",
    "             n_nodes  the number of nodes in the layer\n",
    "             act      specifies the activation function\n",
    "                      Use 'logistic' or 'identity'\n",
    "        '''\n",
    "        self.N = n_nodes  # number of nodes in this layer\n",
    "        self.h = []       # node activities\n",
    "        self.b = np.zeros(self.N)  # biases\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigma = Logistic\n",
    "        self.sigma_p = (lambda : Logistic_p(self.h))\n",
    "        if act=='identity':\n",
    "            self.sigma = Identity\n",
    "            self.sigma_p = (lambda : Identity_p(self.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "code_folding": [
     2,
     65,
     81
    ]
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self, sizes, type='classifier'):\n",
    "        '''\n",
    "            net = Network(sizes, type='classifier')\n",
    "\n",
    "            Creates a Network and saves it in the variable 'net'.\n",
    "\n",
    "            Inputs:\n",
    "              sizes is a list of integers specifying the number\n",
    "                  of nodes in each layer\n",
    "                  eg. [5, 20, 3] will create a 3-layer network\n",
    "                      with 5 input, 20 hidden, and 3 output nodes\n",
    "              type can be either 'classifier' or 'regression', and\n",
    "                  sets the activation function on the output layer,\n",
    "                  as well as the loss function.\n",
    "                  'classifier': logistic, cross entropy\n",
    "                  'regression': linear, mean squared error\n",
    "        '''\n",
    "        self.n_layers = len(sizes)\n",
    "        self.lyr = []    # a list of Layers\n",
    "        self.W = []      # Weight matrices, indexed by the layer below it\n",
    "        \n",
    "        # Two common types of networks\n",
    "        # The member variable self.Loss refers to one of the implemented\n",
    "        # loss functions: MSE, or CrossEntropy.\n",
    "        # Call it using self.Loss(t)\n",
    "        if type=='classifier':\n",
    "            self.classifier = True\n",
    "            self.Loss = self.CrossEntropy\n",
    "            activation = 'logistic'\n",
    "        else:\n",
    "            self.classifier = False\n",
    "            self.Loss = self.MSE\n",
    "            activation = 'identity'\n",
    "\n",
    "        # Create and add Layers (using logistic for hidden layers)\n",
    "        for n in sizes[:-1]:\n",
    "            self.lyr.append( Layer(n) )\n",
    "   \n",
    "        # For the top layer, we use the appropriate activtaion function\n",
    "        self.lyr.append( Layer(sizes[-1], act=activation) )\n",
    "    \n",
    "        # Randomly initialize weight matrices\n",
    "        for idx in range(self.n_layers-1):\n",
    "            m = self.lyr[idx].N\n",
    "            n = self.lyr[idx+1].N\n",
    "            temp = np.random.normal(size=[m,n])/np.sqrt(m)\n",
    "            self.W.append(temp)\n",
    "\n",
    "\n",
    "    def FeedForward(self, x):\n",
    "        '''\n",
    "            y = net.FeedForward(x)\n",
    "\n",
    "            Runs the network forward, starting with x as input.\n",
    "            Returns the activity of the output layer.\n",
    "        '''\n",
    "        x = np.array(x)  # Convert input to array, in case it's not\n",
    "        \n",
    "        # TODO: Check the code for biases.\n",
    "        \n",
    "        # layer.h will be of the shape : [num_samples, num_nodes]\n",
    "        for i in range(len(self.lyr)):\n",
    "            if i == 0:\n",
    "                self.lyr[i].h = x\n",
    "            else:\n",
    "                last_layer_output = self.lyr[i-1].h\n",
    "                linear = np.dot(last_layer_output, self.W[i-1])\n",
    "                linear += np.outer(np.ones(len(x)), self.lyr[i].b)\n",
    "                self.lyr[i].h = self.lyr[i].sigma(linear)\n",
    "                \n",
    "        \n",
    "        return self.lyr[-1].h\n",
    "\n",
    "    \n",
    "    def Evaluate(self, inputs, targets):\n",
    "        '''\n",
    "            E = net.Evaluate(data)\n",
    "\n",
    "            Computes the average loss over the supplied dataset.\n",
    "\n",
    "            Inputs\n",
    "             inputs  is an array of inputs\n",
    "             targets is a list of corresponding targets\n",
    "\n",
    "            Outputs\n",
    "             E is a scalar, the average loss\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        return self.Loss(targets)\n",
    "\n",
    "    def ClassificationAccuracy(self, inputs, targets):\n",
    "        '''\n",
    "            a = net.ClassificationAccuracy(data)\n",
    "            \n",
    "            Returns the fraction (between 0 and 1) of correct one-hot classifications\n",
    "            in the dataset.\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        yb = OneHot(y)\n",
    "        n_incorrect = np.sum(yb!=targets) / 2.\n",
    "        return 1. - float(n_incorrect) / NSamples(inputs)\n",
    "\n",
    "    \n",
    "    def CrossEntropy(self, t):\n",
    "        '''\n",
    "            E = net.CrossEntropy(t)\n",
    "\n",
    "            Evaluates the mean cross entropy loss between t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        output_activity = self.lyr[-1].h # shape = (num_samples, num_outputs)\n",
    "        #shape of t: (num_samples, num_outputs)\n",
    "        \n",
    "        # Equation for Cross Entropy (or negative log likelihood for observing the data given the parameters)\n",
    "        # - (ylog(y) + (1-y)log(1-p))\n",
    "        # Take the mean\n",
    "        result = 0\n",
    "        for i in range(len(t)):\n",
    "            for j in range(5):\n",
    "                if t[i][j] == 1:\n",
    "                        result += np.log(output_activity[i][j])\n",
    "                else:\n",
    "                        result += np.log(1.-output_activity[i][j])\n",
    "        return (-1*result)/len(t)\n",
    "\n",
    "    \n",
    "    def MSE(self, t):\n",
    "        '''\n",
    "            E = net.MSE(t)\n",
    "\n",
    "            Evaluates the MSE loss function using t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        output_activity = self.lyr[-1].h.reshape(-1)\n",
    "        error = np.mean(np.square(output_activity - t))\n",
    "        \n",
    "        return error\n",
    "\n",
    "    \n",
    "    def BackProp(self, t, lrate=0.05):\n",
    "        '''\n",
    "            net.BackProp(targets, lrate=0.05)\n",
    "            \n",
    "            Given the current network state and targets t, updates the connection\n",
    "            weights and biases using the backpropagation algorithm.\n",
    "            \n",
    "            Inputs:\n",
    "             t      an array of targets (number of samples must match the\n",
    "                    network's output)\n",
    "             lrate  learning rate\n",
    "        '''\n",
    "        t = np.array(t)  # convert t to an array, in case it's not\n",
    "        #TODO: Biases\n",
    "        \n",
    "        # We are following (i-1) -> i at index i\n",
    "        for i in range(len(self.lyr)-1,0,-1):\n",
    "            if i == len(self.lyr)-1:\n",
    "                de_dz = (self.lyr[-1].h - t).T # The gradient w.r.t. to the outermost layer: Y x P\n",
    "#                 print(\"DEDZ\")\n",
    "#                 print(de_dz)\n",
    "            else:\n",
    "                dh_dz = Logistic_p(self.lyr[i].h).T # This is H x P\n",
    "                # W[i] is H x Y, de_dz is Y x P\n",
    "                de_dz = np.multiply(dh_dz, np.dot(self.W[i], de_dz)) # This will be H x P\n",
    "            \n",
    "#             print(\"Next Iteration\")\n",
    "#             print(de_dz.shape)\n",
    "            # lyr[i-1].h is P x H, de_dz is Y x P or H(i+1) x P\n",
    "            de_dw = np.dot(de_dz, self.lyr[i-1].h).T # To simplify : This will be H x Y\n",
    "            de_dw /= t.shape[0]\n",
    "#             print(de_dz.shape)\n",
    "#             print(self.lyr[i-1].h.shape)\n",
    "#             print(de_dw.shape)\n",
    "#             print(self.W[i-1].shape)\n",
    "#             print(\"Done\")\n",
    "#             print(\"DEDW\")\n",
    "#             print(de_dw)\n",
    "            self.W[i-1] -= lrate*de_dw\n",
    "#             print(self.W[i-1])\n",
    "        \n",
    "\n",
    "    def Learn(self, inputs, targets, lrate=0.05, epochs=1):\n",
    "        '''\n",
    "            Network.Learn(inputs, targets, lrate=0.05, epochs=1)\n",
    "\n",
    "            Run through the dataset 'epochs' number of times, incrementing the\n",
    "            network weights for each training sample. For each epoch, it\n",
    "            shuffles the order of the samples.\n",
    "\n",
    "            Inputs:\n",
    "              inputs  is an array of input samples\n",
    "              targets is a corresponding array of targets\n",
    "              lrate   is the learning rate (try 0.001 to 0.5)\n",
    "              epochs  is the number of times to go through the training data\n",
    "        '''\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            train_inputs, train_targets = Shuffle(inputs, targets)\n",
    "            y = self.FeedForward(train_inputs)\n",
    "            print(\"Y\")\n",
    "            print(y)\n",
    "            print(\"Training Loss: %.5f\" % self.Loss(train_targets))\n",
    "            self.BackProp(train_targets, lrate)\n",
    "            \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 5 Classes in 8-Dimensional Space\n",
    "np.random.seed(15)\n",
    "noise = 0.1\n",
    "InputClasses = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "OutputClasses = np.array([[1,0,0,0,0],\n",
    "                          [0,1,0,0,0],\n",
    "                          [0,0,1,0,0],\n",
    "                          [0,0,0,1,0],\n",
    "                          [0,0,0,0,1]], dtype=float)\n",
    "n_input = np.shape(InputClasses)[1]\n",
    "n_output = np.shape(OutputClasses)[1]\n",
    "n_classes = np.shape(InputClasses)[0]\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 100\n",
    "training_output = []\n",
    "training_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    training_input.append(x)\n",
    "    training_output.append(t)\n",
    "\n",
    "# Create a test dataset\n",
    "n_samples = 100\n",
    "test_output = []\n",
    "test_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    test_input.append(x)\n",
    "    test_output.append(t)\n",
    "\n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 3.617051325333447\n",
      "     Accuracy = 26.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST BOX\n",
    "import Network_solutions as solution\n",
    "net2 = solution.Network([n_input, 18, n_output], type='classifier')\n",
    "net2.W = net.W.copy()\n",
    "CE = net2.Evaluate(train[0], train[1])\n",
    "accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "print(net2.W[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# net.Learn(train[0], train[1], epochs=500, lrate=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAIN TESTING MULTIPLE RUNS ####\n",
    "input_train, input_target = Shuffle(train[0], train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 5, n_output], type='classifier')\n",
    "\n",
    "#TESTING BOX\n",
    "import Network_solutions as solution\n",
    "net_solution = solution.Network([n_input, 5, n_output], type='classifier')\n",
    "import copy\n",
    "net_solution.W = copy.deepcopy(net.W)\n",
    "print(len(net.W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 3.162815392999932\n",
      "     Accuracy = 34.0%\n",
      "[[ 0.05820378  0.11484255  0.79227401 -0.35714984 -0.3404906 ]\n",
      " [ 0.04268808  0.26792493  0.01418548  0.80275736 -0.08161449]\n",
      " [-0.54954567 -0.57460021 -0.36380406 -0.59753109  0.39257261]\n",
      " [ 0.27980367 -0.03104017  0.1842125   0.02590733  0.08978368]\n",
      " [-0.35151415 -0.46586007 -0.37557225 -0.76925905 -0.07638572]]\n",
      "[[ 0.05385679  0.10607211  0.78613672 -0.35958471 -0.34980938]\n",
      " [ 0.03558169  0.2613582   0.01030249  0.79676027 -0.09153844]\n",
      " [-0.5570662  -0.58356683 -0.36990761 -0.60374525  0.38108713]\n",
      " [ 0.27400821 -0.03619155  0.18304317  0.02095198  0.08172923]\n",
      " [-0.35826155 -0.46962605 -0.38152087 -0.7755291  -0.08029861]]\n"
     ]
    }
   ],
   "source": [
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "\n",
    "y = net.FeedForward(input_train)\n",
    "print(net.W[-1])\n",
    "net.BackProp(input_target)\n",
    "print(net.W[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 4.423854000697025\n",
      "     Accuracy = 18.999999999999993%\n",
      "[[ 0.20874109  0.26589668 -0.41692938  0.23924772  0.25715379]\n",
      " [-0.06930413 -0.49236972  0.11306779  0.02651953 -0.10656793]\n",
      " [ 0.06796996  0.24820616  1.31167325  0.43317715  0.02493608]\n",
      " [ 0.53935371  0.63531349 -0.368591    0.32082186  0.23490489]\n",
      " [-0.03372971 -0.25875635 -0.80325049  0.97744912  0.33968237]]\n",
      "[[ 0.19412418  0.24940751 -0.42598344  0.21967039  0.24294156]\n",
      " [-0.07335174 -0.50145348  0.10535919  0.01548295 -0.11402645]\n",
      " [ 0.05702615  0.23279607  1.30489739  0.41577995  0.0108143 ]\n",
      " [ 0.52830506  0.62593738 -0.37410901  0.30601306  0.22484897]\n",
      " [-0.0444704  -0.27030687 -0.80919895  0.96201717  0.32948376]]\n"
     ]
    }
   ],
   "source": [
    "CE = net_solution.Evaluate(train[0], train[1])\n",
    "accuracy = net_solution.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "\n",
    "print(net_solution.W[-1])\n",
    "y = net_solution.FeedForward(input_train)\n",
    "net_solution.BackProp(input_target)\n",
    "print(net_solution.W[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Training Set')\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## You can also try using the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import Network\n",
    "# importlib.reload(Network)\n",
    "# net2 = Network.Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# net2.Learn(train[0], train[1], epochs=500, lrate=1.)\n",
    "# print('Training Set')\n",
    "# CE = net2.Evaluate(train[0], train[1])\n",
    "# accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "# print('Cross Entropy = '+str(CE))\n",
    "# print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "# print('Test Set')\n",
    "# CE = net2.Evaluate(test[0], test[1])\n",
    "# accuracy = net2.ClassificationAccuracy(test[0], test[1])\n",
    "# print('Cross Entropy = '+str(CE))\n",
    "# print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1D -> 1D (linear mapping)\n",
    "np.random.seed(846)\n",
    "n_input = 1\n",
    "n_output = 1\n",
    "slope = np.random.rand() - 0.5\n",
    "intercept = np.random.rand()*2. - 1.\n",
    "\n",
    "def myfunc(x):\n",
    "    return slope*x+intercept\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 200\n",
    "training_output = []\n",
    "training_input = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    training_input.append(np.array([x]))\n",
    "    training_output.append(np.array([t]))\n",
    "\n",
    "# Create a testing dataset\n",
    "n_samples = 50\n",
    "test_input = []\n",
    "test_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx] + np.random.normal(scale=0.1)\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    test_input.append(np.array([x]))\n",
    "    test_output.append(np.array([t]))\n",
    "\n",
    "# Create a perfect dataset\n",
    "n_samples = 100\n",
    "perfect_input = []\n",
    "perfect_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x)\n",
    "    perfect_input.append(np.array([x]))\n",
    "    perfect_output.append(np.array([t]))\n",
    "    \n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]\n",
    "perfect = [np.array(perfect_input), np.array(perfect_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = Network([1, 10, 1], type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate it before training\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# On training dataset\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('Training MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# On test dataset\n",
    "mse = net.Evaluate(test[0], test[1])\n",
    "print('Test MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model and the TRUE solution (since we know it)\n",
    "s = np.linspace(-1, 1, 200)\n",
    "y = net.FeedForward(np.array([s]).T)\n",
    "p = [myfunc(x) for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the training data,\n",
    "# as well as out model and the true model\n",
    "plt.plot(s,y, 'r--')\n",
    "plt.plot(s,p, 'g:')\n",
    "plt.plot(training_input, training_output, 'bo')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = t.copy()\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t[0] = 2\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
