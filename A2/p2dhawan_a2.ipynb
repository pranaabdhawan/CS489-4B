{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOU_a2 (v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q1: Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer here (double-click)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To help you with $\\LaTeX$, and to show you my expectations, here is a sample taken from the lecture notes, taken from the 3rd and 4th page of the notes entitled \"Error Backpropagation\". It has nothing to do with the solution to this question, but just demonstrates some of the features of $\\LaTeX$. Notice how I include English statments to guide the reader through the derivation.\n",
    "\n",
    "<a target=_new href=\"http://detexify.kirelabs.org/classify.html\">This web page</a> is very handy for identifying $\\LaTeX$ symbols.\n",
    "\n",
    "---\n",
    "More generally, for $\\vec{x} \\in \\mathbb{R}^X$, $\\vec{h} \\in \\mathbb{R}^H$, and $\\vec{y} \\in \\mathbb{R}^Y$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\alpha_i}\n",
    "  &= \\frac{d h_i}{d \\alpha_i} \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "  \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right] \\cdot\n",
    "  \\left[ \\frac{\\partial E}{\\partial \\beta_1} \\ \\cdots \\ \\frac{\\partial E}{\\partial \\beta_Y} \\right] \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "   \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right]\n",
    "   \\left[ \\begin{array}{c}\n",
    "     \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\frac{\\partial E}{\\partial \\beta_Y} \\end{array} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, for all elements,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\alpha_H}\n",
    "\\end{array} \\right] &=\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{d h_1}{d \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{d h_H}{d \\alpha_H}\n",
    "\\end{array} \\right]\n",
    "\\odot\n",
    "\\left[ \\begin{array}{ccc}\n",
    "  M_{11} & \\cdots & M_{Y1} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  M_{1H} & \\cdots & M_{YH}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\beta_Y}\n",
    "\\end{array} \\right] \\\\\n",
    "%\n",
    "\\frac{\\partial E}{\\partial \\vec{\\alpha}} &=\n",
    "\\frac{d \\vec{h}}{d \\vec{\\alpha}} \\odot M^\\mathrm{T}\n",
    "\\frac{\\partial E}{\\partial \\vec{\\beta}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q3: Top-Layer Error Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q4: Implementing Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Supplied Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "code_folding": [
     16,
     35
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Supplied functions\n",
    "\n",
    "def NSamples(x):\n",
    "    '''\n",
    "        n = NSamples(x)\n",
    "        \n",
    "        Returns the number of samples in a batch of inputs.\n",
    "        \n",
    "        Input:\n",
    "         x   is a 2D array\n",
    "        \n",
    "        Output:\n",
    "         n   is an integer\n",
    "    '''\n",
    "    return len(x)\n",
    "\n",
    "def Shuffle(inputs, targets):\n",
    "    '''\n",
    "        s_inputs, s_targets = Shuffle(inputs, targets)\n",
    "        \n",
    "        Randomly shuffles the dataset.\n",
    "        \n",
    "        Inputs:\n",
    "         inputs     array of inputs\n",
    "         targets    array of corresponding targets\n",
    "         \n",
    "        Outputs:\n",
    "         s_inputs   shuffled array of inputs\n",
    "         s_targets  corresponding shuffled array of targets\n",
    "    '''\n",
    "    data = list(zip(inputs,targets))\n",
    "    np.random.shuffle(data)\n",
    "    s_inputs, s_targets = zip(*data)\n",
    "    return np.array(s_inputs), np.array(s_targets)\n",
    "\n",
    "def Logistic(z):\n",
    "    '''\n",
    "        y = Logistic(z)\n",
    "\n",
    "        Applies the logistic function to each element in z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return 1. / (1 + np.exp(-z) )\n",
    "\n",
    "def Logistic_p(h):\n",
    "    '''\n",
    "        yp = Logistic_p(h)\n",
    "        \n",
    "        Returns the slope of the logistic function at z when h = Logistic(z).\n",
    "        Note the h is the input, NOT z.\n",
    "    '''\n",
    "    return h*(1.-h)\n",
    "\n",
    "def Identity(z):\n",
    "    '''\n",
    "        y = Identity(z)\n",
    "\n",
    "        Does nothing... simply returns z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return z\n",
    "\n",
    "def Identity_p(h):\n",
    "    '''\n",
    "        yp = Identity_p(h)\n",
    "        \n",
    "        Returns the slope of the identity function h.\n",
    "    '''\n",
    "    return np.ones_like(h)\n",
    "\n",
    "def OneHot(z):\n",
    "    '''\n",
    "        y = OneHot(z)\n",
    "\n",
    "        Applies the one-hot function to the vectors in z.\n",
    "        Example:\n",
    "          OneHot([[0.9, 0.1], [-0.5, 0.1]])\n",
    "          returns np.array([[1,0],[0,1]])\n",
    "\n",
    "        Input:\n",
    "         z    is a 2D array of samples\n",
    "\n",
    "        Output:\n",
    "         y    is an array the same shape as z\n",
    "    '''\n",
    "    y = []\n",
    "    # Locate the max of each row\n",
    "    for zz in z:\n",
    "        idx = np.argmax(zz)\n",
    "        b = np.zeros_like(zz)\n",
    "        b[idx] = 1.\n",
    "        y.append(b)\n",
    "    y = np.array(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def __init__(self, n_nodes, act='logistic'):\n",
    "        '''\n",
    "            lyr = Layer(n_nodes, act='logistic')\n",
    "            \n",
    "            Creates a layer object.\n",
    "            \n",
    "            Inputs:\n",
    "             n_nodes  the number of nodes in the layer\n",
    "             act      specifies the activation function\n",
    "                      Use 'logistic' or 'identity'\n",
    "        '''\n",
    "        self.N = n_nodes  # number of nodes in this layer\n",
    "        self.h = []       # node activities\n",
    "        self.b = np.zeros(self.N)  # biases\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigma = Logistic\n",
    "        self.sigma_p = (lambda : Logistic_p(self.h))\n",
    "        if act=='identity':\n",
    "            self.sigma = Identity\n",
    "            self.sigma_p = (lambda : Identity_p(self.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {
    "code_folding": [
     2,
     65,
     81
    ]
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self, sizes, type='classifier'):\n",
    "        '''\n",
    "            net = Network(sizes, type='classifier')\n",
    "\n",
    "            Creates a Network and saves it in the variable 'net'.\n",
    "\n",
    "            Inputs:\n",
    "              sizes is a list of integers specifying the number\n",
    "                  of nodes in each layer\n",
    "                  eg. [5, 20, 3] will create a 3-layer network\n",
    "                      with 5 input, 20 hidden, and 3 output nodes\n",
    "              type can be either 'classifier' or 'regression', and\n",
    "                  sets the activation function on the output layer,\n",
    "                  as well as the loss function.\n",
    "                  'classifier': logistic, cross entropy\n",
    "                  'regression': linear, mean squared error\n",
    "        '''\n",
    "        self.n_layers = len(sizes)\n",
    "        self.lyr = []    # a list of Layers\n",
    "        self.W = []      # Weight matrices, indexed by the layer below it\n",
    "        \n",
    "        # Two common types of networks\n",
    "        # The member variable self.Loss refers to one of the implemented\n",
    "        # loss functions: MSE, or CrossEntropy.\n",
    "        # Call it using self.Loss(t)\n",
    "        if type=='classifier':\n",
    "            self.classifier = True\n",
    "            self.Loss = self.CrossEntropy\n",
    "            activation = 'logistic'\n",
    "        else:\n",
    "            self.classifier = False\n",
    "            self.Loss = self.MSE\n",
    "            activation = 'identity'\n",
    "\n",
    "        # Create and add Layers (using logistic for hidden layers)\n",
    "        for n in sizes[:-1]:\n",
    "            self.lyr.append( Layer(n) )\n",
    "   \n",
    "        # For the top layer, we use the appropriate activtaion function\n",
    "        self.lyr.append( Layer(sizes[-1], act=activation) )\n",
    "    \n",
    "        # Randomly initialize weight matrices\n",
    "        for idx in range(self.n_layers-1):\n",
    "            m = self.lyr[idx].N\n",
    "            n = self.lyr[idx+1].N\n",
    "            temp = np.random.normal(size=[m,n])/np.sqrt(m)\n",
    "            self.W.append(temp)\n",
    "\n",
    "\n",
    "    def FeedForward(self, x):\n",
    "        '''\n",
    "            y = net.FeedForward(x)\n",
    "\n",
    "            Runs the network forward, starting with x as input.\n",
    "            Returns the activity of the output layer.\n",
    "        '''\n",
    "        x = np.array(x)  # Convert input to array, in case it's not\n",
    "        \n",
    "        # TODO: Check the code for biases.\n",
    "        \n",
    "        # layer.h will be of the shape : [num_samples, num_nodes]\n",
    "        for i in range(len(self.lyr)):\n",
    "            if i == 0:\n",
    "                self.lyr[i].h = x\n",
    "            else:\n",
    "                last_layer_output = self.lyr[i-1].h\n",
    "                linear = np.dot(last_layer_output, self.W[i-1])\n",
    "                linear += np.outer(np.ones(len(x)), self.lyr[i].b)\n",
    "                self.lyr[i].h = self.lyr[i].sigma(linear)\n",
    "                \n",
    "        \n",
    "        return self.lyr[-1].h\n",
    "\n",
    "    \n",
    "    def Evaluate(self, inputs, targets):\n",
    "        '''\n",
    "            E = net.Evaluate(data)\n",
    "\n",
    "            Computes the average loss over the supplied dataset.\n",
    "\n",
    "            Inputs\n",
    "             inputs  is an array of inputs\n",
    "             targets is a list of corresponding targets\n",
    "\n",
    "            Outputs\n",
    "             E is a scalar, the average loss\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        return self.Loss(targets)\n",
    "\n",
    "    def ClassificationAccuracy(self, inputs, targets):\n",
    "        '''\n",
    "            a = net.ClassificationAccuracy(data)\n",
    "            \n",
    "            Returns the fraction (between 0 and 1) of correct one-hot classifications\n",
    "            in the dataset.\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        yb = OneHot(y)\n",
    "        n_incorrect = np.sum(yb!=targets) / 2.\n",
    "        return 1. - float(n_incorrect) / NSamples(inputs)\n",
    "\n",
    "    \n",
    "    def CrossEntropy(self, t):\n",
    "        '''\n",
    "            E = net.CrossEntropy(t)\n",
    "\n",
    "            Evaluates the mean cross entropy loss between t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        output_activity = self.lyr[-1].h # shape = (num_samples, num_outputs)\n",
    "        #shape of t: (num_samples, num_outputs)\n",
    "        \n",
    "        # Equation for Cross Entropy (or negative log likelihood for observing the data given the parameters)\n",
    "        # - (ylog(y) + (1-y)log(1-p))\n",
    "        # Take the mean\n",
    "        result = 0\n",
    "        for i in range(len(t)):\n",
    "            for j in range(5):\n",
    "                if t[i][j] == 1:\n",
    "                        result += np.log(output_activity[i][j])\n",
    "                else:\n",
    "                        result += np.log(1.-output_activity[i][j])\n",
    "        return (-1*result)/len(t)\n",
    "\n",
    "    \n",
    "    def MSE(self, t):\n",
    "        '''\n",
    "            E = net.MSE(t)\n",
    "\n",
    "            Evaluates the MSE loss function using t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        output_activity = self.lyr[-1].h.reshape(-1)\n",
    "        error = np.mean(np.square(output_activity - t))\n",
    "        \n",
    "        return error\n",
    "\n",
    "    \n",
    "    def BackProp(self, t, lrate=0.05):\n",
    "        '''\n",
    "            net.BackProp(targets, lrate=0.05)\n",
    "            \n",
    "            Given the current network state and targets t, updates the connection\n",
    "            weights and biases using the backpropagation algorithm.\n",
    "            \n",
    "            Inputs:\n",
    "             t      an array of targets (number of samples must match the\n",
    "                    network's output)\n",
    "             lrate  learning rate\n",
    "        '''\n",
    "        t = np.array(t)  # convert t to an array, in case it's not\n",
    "        #TODO: Biases\n",
    "        \n",
    "        # We are following (i-1) -> i at index i\n",
    "        for i in range(len(self.lyr)-1,0,-1):\n",
    "            if i == len(self.lyr)-1:\n",
    "                de_dz = (self.lyr[-1].h - t).T # The gradient w.r.t. to the outermost layer: Y x P\n",
    "            else:\n",
    "                dh_dz = Logistic_p(self.lyr[i].h).T # This is H x P\n",
    "                # W[i] is H x Y, de_dz is Y x P\n",
    "                de_dz = np.multiply(dh_dz, np.dot(self.W[i], de_dz)) # This will be H x P\n",
    "            \n",
    "#             print(\"Next Iteration\")\n",
    "#             print(de_dz.shape)\n",
    "            # lyr[i-1].h is P x H, de_dz is Y x P or H(i+1) x P\n",
    "            de_dw = np.dot(de_dz, self.lyr[i-1].h).T # To simplify : This will be H x Y\n",
    "#             print(de_dw.shape)\n",
    "            self.W[i-1] += lrate*de_dw\n",
    "#             print(self.W[i-1])\n",
    "        \n",
    "\n",
    "    def Learn(self, inputs, targets, lrate=0.05, epochs=1):\n",
    "        '''\n",
    "            Network.Learn(inputs, targets, lrate=0.05, epochs=1)\n",
    "\n",
    "            Run through the dataset 'epochs' number of times, incrementing the\n",
    "            network weights for each training sample. For each epoch, it\n",
    "            shuffles the order of the samples.\n",
    "\n",
    "            Inputs:\n",
    "              inputs  is an array of input samples\n",
    "              targets is a corresponding array of targets\n",
    "              lrate   is the learning rate (try 0.001 to 0.5)\n",
    "              epochs  is the number of times to go through the training data\n",
    "        '''\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            train_inputs, train_targets = Shuffle(inputs, targets)\n",
    "            y = self.FeedForward(train_inputs)\n",
    "            print(\"Y\")\n",
    "            print(y)\n",
    "            print(\"Training Loss: %.5f\" % self.Loss(train_targets))\n",
    "            self.BackProp(train_targets, lrate)\n",
    "            \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 5 Classes in 8-Dimensional Space\n",
    "np.random.seed(15)\n",
    "noise = 0.1\n",
    "InputClasses = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "OutputClasses = np.array([[1,0,0,0,0],\n",
    "                          [0,1,0,0,0],\n",
    "                          [0,0,1,0,0],\n",
    "                          [0,0,0,1,0],\n",
    "                          [0,0,0,0,1]], dtype=float)\n",
    "n_input = np.shape(InputClasses)[1]\n",
    "n_output = np.shape(OutputClasses)[1]\n",
    "n_classes = np.shape(InputClasses)[0]\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 100\n",
    "training_output = []\n",
    "training_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    training_input.append(x)\n",
    "    training_output.append(t)\n",
    "\n",
    "# Create a test dataset\n",
    "n_samples = 100\n",
    "test_output = []\n",
    "test_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    test_input.append(x)\n",
    "    test_output.append(t)\n",
    "\n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 3.577892835321577\n",
      "     Accuracy = 19.999999999999996%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 46.84236554431104\n",
      "     Accuracy = 12.0%\n",
      "(8, 18)\n"
     ]
    }
   ],
   "source": [
    "# TEST BOX\n",
    "import Network_solutions as solution\n",
    "net2 = solution.Network([n_input, 18, n_output], type='classifier')\n",
    "net2.W = net.W.copy()\n",
    "CE = net2.Evaluate(train[0], train[1])\n",
    "accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "print(net2.W[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=500, lrate=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TESTING BOX\n",
    "net.FeedForward(train[0])\n",
    "net.BackProp(train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MAIN TESTING MULTIPLE RUNS ####\n",
    "input_train, input_target = Shuffle(train[0], train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 18, n_output], type='classifier')\n",
    "\n",
    "#TESTING BOX\n",
    "import Network_solutions as solution\n",
    "net2 = solution.Network([n_input, 18, n_output], type='classifier')\n",
    "net2.W = net.W.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "[array([[-0.08269516,  0.28603147, -0.34324139,  0.3395778 , -0.2374235 ,\n",
      "         0.14368966,  0.011261  , -0.05923884,  0.35132725, -0.27011754,\n",
      "        -0.20410358, -0.19010997,  0.00457226, -0.37529071,  0.68603327,\n",
      "        -0.11549121,  0.10046357, -0.74853926],\n",
      "       [ 0.20926747, -0.47297272,  0.25244951, -0.21710999,  0.02648995,\n",
      "         0.10561785, -0.50185002, -0.2190081 ,  0.16078679,  0.16196199,\n",
      "        -0.01355109, -0.09095244,  0.29911497, -0.09647553,  0.00694064,\n",
      "         0.08482302,  0.01055077,  0.18946303],\n",
      "       [ 0.23611057, -0.19132892, -0.29248902,  0.05561743, -0.36959791,\n",
      "         0.33191658,  0.21449166,  0.49484617,  0.0196261 ,  0.10325156,\n",
      "        -0.26293746, -0.04991183,  0.06478041, -0.19891466, -0.07019049,\n",
      "        -0.41937896,  0.18511717,  0.01345264],\n",
      "       [ 0.16525322, -0.11647937, -0.47086537,  0.24835535,  0.15146675,\n",
      "        -0.8128349 ,  0.54781113, -0.45819026, -0.38813314, -0.14436126,\n",
      "        -0.21564004,  0.10342172, -0.29648189,  0.92059607, -0.07408564,\n",
      "        -0.345751  ,  0.18546476,  0.54645981],\n",
      "       [ 0.5180701 ,  0.05696122,  0.21483863, -0.4676297 , -0.0074116 ,\n",
      "        -0.52261891,  0.23103332,  0.18849893, -0.17446955, -0.03565467,\n",
      "        -0.17530823,  0.23680454, -0.15656481,  0.27534831, -0.24988643,\n",
      "        -0.0704366 ,  0.06090326,  0.20157968],\n",
      "       [-0.13323358,  0.83061889, -0.02361336, -0.36678739, -0.08668798,\n",
      "        -0.45308272,  0.19840687,  0.29669977, -0.21203733, -0.38751893,\n",
      "        -0.87747591, -0.23349047, -0.10198932, -0.20596658,  0.01608402,\n",
      "        -0.96514098, -0.22837781,  0.29085553],\n",
      "       [ 0.10382666, -0.24919435, -0.1383076 , -0.15191772,  0.24617956,\n",
      "        -0.22345386,  0.00934974,  0.03372861, -0.52266563, -0.65957727,\n",
      "         0.32591231,  0.07895321, -0.1937798 ,  0.49969967, -0.50183091,\n",
      "        -0.34713496,  0.28732725, -0.35910924],\n",
      "       [ 0.16697549,  0.35331211, -0.03701062, -0.36352354, -0.35281794,\n",
      "         0.21529334, -0.16096302, -0.48002923,  0.15491277,  0.25081364,\n",
      "        -0.13580424, -0.37684385,  0.45119811,  0.17325095,  0.27301175,\n",
      "        -0.53603795, -0.21892677, -0.82581118]]), array([[-0.25086971, -0.04537454, -0.39616601, -0.24196088,  0.0862131 ],\n",
      "       [ 0.28651991, -0.10013895, -0.12268616,  0.07835678, -0.09374067],\n",
      "       [-0.10559192,  0.20307111, -0.07105459, -0.20564596, -0.07883194],\n",
      "       [-0.0966786 ,  0.07200824, -0.41767067,  0.26353699,  0.10353078],\n",
      "       [-0.03067051, -0.30482538, -0.10705662,  0.18126036, -0.30907956],\n",
      "       [ 0.13855065,  0.0598656 ,  0.14626362, -0.08228732, -0.10864329],\n",
      "       [-0.03468811, -0.10517374, -0.26532081,  0.10260324, -0.35190884],\n",
      "       [-0.01226271,  0.0579035 ,  0.36500104,  0.06230981,  0.00788795],\n",
      "       [-0.02677896, -0.01531375, -0.03813886, -0.04853477,  0.18951386],\n",
      "       [ 0.39717969, -0.10674932,  0.22887367, -0.03180475, -0.18437924],\n",
      "       [-0.17283772, -0.0203344 ,  0.23896411, -0.21299087, -0.24760646],\n",
      "       [-0.42792156,  0.07385338,  0.26221645,  0.21468861, -0.06727671],\n",
      "       [-0.24834622,  0.36912386, -0.17732672, -0.062827  ,  0.31830533],\n",
      "       [-0.12016401,  0.11439234, -0.11822181, -0.11607772, -0.27388339],\n",
      "       [-0.43663344, -0.01545657, -0.0547891 ,  0.01786402,  0.35718545],\n",
      "       [-0.12694513,  0.34954682, -0.09802307,  0.01808424, -0.04962326],\n",
      "       [ 0.19765723,  0.49587267, -0.07770707, -0.00423843, -0.00133114],\n",
      "       [-0.00773267,  0.26319491, -0.22311331, -0.01168742, -0.35250971]])]\n"
     ]
    }
   ],
   "source": [
    "net.W = np.ones(10)\n",
    "print(net.W)\n",
    "print(net2.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "[array([[-0.04013366,  1.33730562,  0.65302784, -0.16861244,  0.65507931,\n",
      "         0.92303452,  1.70745016,  1.38748254,  1.1303832 ,  0.05223658,\n",
      "         1.6601683 ,  1.02555026,  0.76422594,  0.73905697,  0.46727726,\n",
      "         1.10881584,  1.39047443,  0.45900358],\n",
      "       [ 0.14044818,  0.62958561, -0.10481423,  0.76748008,  0.67318431,\n",
      "         0.48489608,  0.44149768, -0.3800605 ,  0.69331358,  0.20716951,\n",
      "         0.73882529,  0.18931169,  0.27427759,  0.6185605 ,  0.94087309,\n",
      "         0.02152397,  0.75168928,  0.40762717],\n",
      "       [-0.65076443,  1.21731843,  0.88684166,  0.44541354,  0.84198288,\n",
      "         0.6109246 ,  1.21956932,  0.71207985,  0.33125817,  0.7947184 ,\n",
      "         1.14804585,  0.78429465,  0.50767533,  0.58541964,  0.92676067,\n",
      "         0.81132738,  1.14848149,  0.67805858],\n",
      "       [ 0.45744583,  0.75988149,  0.38478177,  0.57030389, -0.79451949,\n",
      "         1.04820283,  0.74672025,  0.3752525 ,  0.13234206, -0.03104496,\n",
      "         0.36177015,  0.85555659,  0.06494562,  0.96469149, -0.05577447,\n",
      "        -0.16861316,  0.36160488,  0.59210512],\n",
      "       [-0.07890005,  1.34483172,  0.9032328 ,  0.88667074,  1.10352481,\n",
      "         0.86361187,  0.81918142,  0.1274553 ,  0.86367213,  0.34496451,\n",
      "         1.31070305,  0.6022848 ,  0.243986  , -0.27461687,  0.19200263,\n",
      "         0.57695357,  1.28033059,  0.11403244],\n",
      "       [-0.24670732,  1.38232628, -0.08779164,  0.88622465,  1.09373205,\n",
      "         0.83310941,  0.87887759,  0.82239798,  0.88925597,  0.27410759,\n",
      "         1.36770354,  1.61646458,  0.30203028,  1.12253152,  1.24327855,\n",
      "         0.36580162,  1.00437907,  0.18232484],\n",
      "       [ 0.05062815,  0.66668782,  0.74799883,  0.41750913,  0.61926726,\n",
      "         0.54410093,  0.77264882,  0.54721255,  0.9249063 ,  1.03374593,\n",
      "         0.6081242 ,  0.37173731,  0.99527411,  0.23800905,  0.40249986,\n",
      "         0.7640739 ,  1.03497635, -0.09737915],\n",
      "       [ 0.39831403,  1.27938031,  0.04592106,  0.29724562,  1.20520185,\n",
      "         1.63641855,  1.63466275,  0.52754286,  1.69985445,  0.45994307,\n",
      "         1.15222327,  1.14945207,  0.66085093,  0.78312392,  0.95945873,\n",
      "         1.09802233,  1.69094815,  0.25534441]]), array([[-0.07687717,  0.53117089,  0.72905904,  0.45512125, -0.01876606],\n",
      "       [ 0.60371933,  1.66073663,  0.72801586,  1.07061754,  0.55943515],\n",
      "       [ 0.00391828,  1.18635421,  0.5358831 ,  0.45323302,  0.46834934],\n",
      "       [ 0.37914658,  0.98370843, -0.11742849,  0.68347983,  0.35052044],\n",
      "       [ 0.4484811 ,  1.07643032,  0.04634456,  1.15810306,  0.77614692],\n",
      "       [ 0.48451427,  1.58389955,  0.36563811,  0.86053084,  0.74280322],\n",
      "       [ 0.77026231,  1.85438108,  0.63921488,  0.93527004,  0.47831852],\n",
      "       [ 0.28536301,  1.0626719 ,  0.66454174,  0.60452553,  0.47971035],\n",
      "       [ 0.85858781,  1.15756885,  0.25850598,  0.79566795,  1.41810136],\n",
      "       [ 0.08771571,  1.14868551, -0.27324336,  0.59004664,  0.43732588],\n",
      "       [ 0.30648284,  1.8962709 ,  0.56511712,  1.11908874,  0.29123135],\n",
      "       [ 0.25011015,  1.38367166,  0.80164664,  1.16226816,  0.37958815],\n",
      "       [ 0.08646145,  1.35005544,  0.55750627,  0.25551261,  0.83820453],\n",
      "       [ 0.64835534,  0.89697018,  0.58329972,  1.19522381,  0.51841883],\n",
      "       [ 0.05033676,  1.31797869,  0.46769617,  0.86723215,  0.9498605 ],\n",
      "       [ 0.10645185,  1.2323975 ,  0.29171042,  0.68511529,  0.53419721],\n",
      "       [ 0.68814369,  1.94051166,  0.4801931 ,  0.70988855,  0.95934799],\n",
      "       [ 0.23839321,  1.21042427, -0.02990134,  0.73958138,  0.17187607]])]\n",
      "_--------------------------_\n",
      "[array([[-0.04013366,  1.33730562,  0.65302784, -0.16861244,  0.65507931,\n",
      "         0.92303452,  1.70745016,  1.38748254,  1.1303832 ,  0.05223658,\n",
      "         1.6601683 ,  1.02555026,  0.76422594,  0.73905697,  0.46727726,\n",
      "         1.10881584,  1.39047443,  0.45900358],\n",
      "       [ 0.14044818,  0.62958561, -0.10481423,  0.76748008,  0.67318431,\n",
      "         0.48489608,  0.44149768, -0.3800605 ,  0.69331358,  0.20716951,\n",
      "         0.73882529,  0.18931169,  0.27427759,  0.6185605 ,  0.94087309,\n",
      "         0.02152397,  0.75168928,  0.40762717],\n",
      "       [-0.65076443,  1.21731843,  0.88684166,  0.44541354,  0.84198288,\n",
      "         0.6109246 ,  1.21956932,  0.71207985,  0.33125817,  0.7947184 ,\n",
      "         1.14804585,  0.78429465,  0.50767533,  0.58541964,  0.92676067,\n",
      "         0.81132738,  1.14848149,  0.67805858],\n",
      "       [ 0.45744583,  0.75988149,  0.38478177,  0.57030389, -0.79451949,\n",
      "         1.04820283,  0.74672025,  0.3752525 ,  0.13234206, -0.03104496,\n",
      "         0.36177015,  0.85555659,  0.06494562,  0.96469149, -0.05577447,\n",
      "        -0.16861316,  0.36160488,  0.59210512],\n",
      "       [-0.07890005,  1.34483172,  0.9032328 ,  0.88667074,  1.10352481,\n",
      "         0.86361187,  0.81918142,  0.1274553 ,  0.86367213,  0.34496451,\n",
      "         1.31070305,  0.6022848 ,  0.243986  , -0.27461687,  0.19200263,\n",
      "         0.57695357,  1.28033059,  0.11403244],\n",
      "       [-0.24670732,  1.38232628, -0.08779164,  0.88622465,  1.09373205,\n",
      "         0.83310941,  0.87887759,  0.82239798,  0.88925597,  0.27410759,\n",
      "         1.36770354,  1.61646458,  0.30203028,  1.12253152,  1.24327855,\n",
      "         0.36580162,  1.00437907,  0.18232484],\n",
      "       [ 0.05062815,  0.66668782,  0.74799883,  0.41750913,  0.61926726,\n",
      "         0.54410093,  0.77264882,  0.54721255,  0.9249063 ,  1.03374593,\n",
      "         0.6081242 ,  0.37173731,  0.99527411,  0.23800905,  0.40249986,\n",
      "         0.7640739 ,  1.03497635, -0.09737915],\n",
      "       [ 0.39831403,  1.27938031,  0.04592106,  0.29724562,  1.20520185,\n",
      "         1.63641855,  1.63466275,  0.52754286,  1.69985445,  0.45994307,\n",
      "         1.15222327,  1.14945207,  0.66085093,  0.78312392,  0.95945873,\n",
      "         1.09802233,  1.69094815,  0.25534441]]), array([[-0.07687717,  0.53117089,  0.72905904,  0.45512125, -0.01876606],\n",
      "       [ 0.60371933,  1.66073663,  0.72801586,  1.07061754,  0.55943515],\n",
      "       [ 0.00391828,  1.18635421,  0.5358831 ,  0.45323302,  0.46834934],\n",
      "       [ 0.37914658,  0.98370843, -0.11742849,  0.68347983,  0.35052044],\n",
      "       [ 0.4484811 ,  1.07643032,  0.04634456,  1.15810306,  0.77614692],\n",
      "       [ 0.48451427,  1.58389955,  0.36563811,  0.86053084,  0.74280322],\n",
      "       [ 0.77026231,  1.85438108,  0.63921488,  0.93527004,  0.47831852],\n",
      "       [ 0.28536301,  1.0626719 ,  0.66454174,  0.60452553,  0.47971035],\n",
      "       [ 0.85858781,  1.15756885,  0.25850598,  0.79566795,  1.41810136],\n",
      "       [ 0.08771571,  1.14868551, -0.27324336,  0.59004664,  0.43732588],\n",
      "       [ 0.30648284,  1.8962709 ,  0.56511712,  1.11908874,  0.29123135],\n",
      "       [ 0.25011015,  1.38367166,  0.80164664,  1.16226816,  0.37958815],\n",
      "       [ 0.08646145,  1.35005544,  0.55750627,  0.25551261,  0.83820453],\n",
      "       [ 0.64835534,  0.89697018,  0.58329972,  1.19522381,  0.51841883],\n",
      "       [ 0.05033676,  1.31797869,  0.46769617,  0.86723215,  0.9498605 ],\n",
      "       [ 0.10645185,  1.2323975 ,  0.29171042,  0.68511529,  0.53419721],\n",
      "       [ 0.68814369,  1.94051166,  0.4801931 ,  0.70988855,  0.95934799],\n",
      "       [ 0.23839321,  1.21042427, -0.02990134,  0.73958138,  0.17187607]])]\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(net.W[1], net2.W[1]))\n",
    "print(net.W)\n",
    "print(\"_--------------------------_\")\n",
    "print(net2.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 3.5248994183807048\n",
      "     Accuracy = 12.0%\n",
      "[array([[-0.04013366,  1.33730562,  0.65302784, -0.16861244,  0.65507931,\n",
      "         0.92303452,  1.70745016,  1.38748254,  1.1303832 ,  0.05223658,\n",
      "         1.6601683 ,  1.02555026,  0.76422594,  0.73905697,  0.46727726,\n",
      "         1.10881584,  1.39047443,  0.45900358],\n",
      "       [ 0.14044818,  0.62958561, -0.10481423,  0.76748008,  0.67318431,\n",
      "         0.48489608,  0.44149768, -0.3800605 ,  0.69331358,  0.20716951,\n",
      "         0.73882529,  0.18931169,  0.27427759,  0.6185605 ,  0.94087309,\n",
      "         0.02152397,  0.75168928,  0.40762717],\n",
      "       [-0.65076443,  1.21731843,  0.88684166,  0.44541354,  0.84198288,\n",
      "         0.6109246 ,  1.21956932,  0.71207985,  0.33125817,  0.7947184 ,\n",
      "         1.14804585,  0.78429465,  0.50767533,  0.58541964,  0.92676067,\n",
      "         0.81132738,  1.14848149,  0.67805858],\n",
      "       [ 0.45744583,  0.75988149,  0.38478177,  0.57030389, -0.79451949,\n",
      "         1.04820283,  0.74672025,  0.3752525 ,  0.13234206, -0.03104496,\n",
      "         0.36177015,  0.85555659,  0.06494562,  0.96469149, -0.05577447,\n",
      "        -0.16861316,  0.36160488,  0.59210512],\n",
      "       [-0.07890005,  1.34483172,  0.9032328 ,  0.88667074,  1.10352481,\n",
      "         0.86361187,  0.81918142,  0.1274553 ,  0.86367213,  0.34496451,\n",
      "         1.31070305,  0.6022848 ,  0.243986  , -0.27461687,  0.19200263,\n",
      "         0.57695357,  1.28033059,  0.11403244],\n",
      "       [-0.24670732,  1.38232628, -0.08779164,  0.88622465,  1.09373205,\n",
      "         0.83310941,  0.87887759,  0.82239798,  0.88925597,  0.27410759,\n",
      "         1.36770354,  1.61646458,  0.30203028,  1.12253152,  1.24327855,\n",
      "         0.36580162,  1.00437907,  0.18232484],\n",
      "       [ 0.05062815,  0.66668782,  0.74799883,  0.41750913,  0.61926726,\n",
      "         0.54410093,  0.77264882,  0.54721255,  0.9249063 ,  1.03374593,\n",
      "         0.6081242 ,  0.37173731,  0.99527411,  0.23800905,  0.40249986,\n",
      "         0.7640739 ,  1.03497635, -0.09737915],\n",
      "       [ 0.39831403,  1.27938031,  0.04592106,  0.29724562,  1.20520185,\n",
      "         1.63641855,  1.63466275,  0.52754286,  1.69985445,  0.45994307,\n",
      "         1.15222327,  1.14945207,  0.66085093,  0.78312392,  0.95945873,\n",
      "         1.09802233,  1.69094815,  0.25534441]]), array([[-0.07687717,  0.53117089,  0.72905904,  0.45512125, -0.01876606],\n",
      "       [ 0.60371933,  1.66073663,  0.72801586,  1.07061754,  0.55943515],\n",
      "       [ 0.00391828,  1.18635421,  0.5358831 ,  0.45323302,  0.46834934],\n",
      "       [ 0.37914658,  0.98370843, -0.11742849,  0.68347983,  0.35052044],\n",
      "       [ 0.4484811 ,  1.07643032,  0.04634456,  1.15810306,  0.77614692],\n",
      "       [ 0.48451427,  1.58389955,  0.36563811,  0.86053084,  0.74280322],\n",
      "       [ 0.77026231,  1.85438108,  0.63921488,  0.93527004,  0.47831852],\n",
      "       [ 0.28536301,  1.0626719 ,  0.66454174,  0.60452553,  0.47971035],\n",
      "       [ 0.85858781,  1.15756885,  0.25850598,  0.79566795,  1.41810136],\n",
      "       [ 0.08771571,  1.14868551, -0.27324336,  0.59004664,  0.43732588],\n",
      "       [ 0.30648284,  1.8962709 ,  0.56511712,  1.11908874,  0.29123135],\n",
      "       [ 0.25011015,  1.38367166,  0.80164664,  1.16226816,  0.37958815],\n",
      "       [ 0.08646145,  1.35005544,  0.55750627,  0.25551261,  0.83820453],\n",
      "       [ 0.64835534,  0.89697018,  0.58329972,  1.19522381,  0.51841883],\n",
      "       [ 0.05033676,  1.31797869,  0.46769617,  0.86723215,  0.9498605 ],\n",
      "       [ 0.10645185,  1.2323975 ,  0.29171042,  0.68511529,  0.53419721],\n",
      "       [ 0.68814369,  1.94051166,  0.4801931 ,  0.70988855,  0.95934799],\n",
      "       [ 0.23839321,  1.21042427, -0.02990134,  0.73958138,  0.17187607]])]\n"
     ]
    }
   ],
   "source": [
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "\n",
    "y = net.FeedForward(input_train)\n",
    "net.BackProp(input_target)\n",
    "print(net2.W)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 2.6709517625853447\n",
      "     Accuracy = 0.0%\n"
     ]
    }
   ],
   "source": [
    "CE = net2.Evaluate(train[0], train[1])\n",
    "accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "\n",
    "y = net2.FeedForward(input_train)\n",
    "net2.BackProp(input_target)\n",
    "# print(net2.W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.80255312 -0.37145196  0.48755532 -0.33667181 -0.3780045  -0.45530105\n",
      "   0.32544903 -0.12509503 -0.06355813 -0.21995553  0.13963478 -0.23846263\n",
      "  -0.42214992  0.29852426  0.12025178  0.04304943 -0.74163176  0.25904209]\n",
      " [-0.27446148  0.07462879  0.1567137   0.2544249   0.26416653 -0.785098\n",
      "   1.12323999  0.53742711  0.44374511 -0.14910145 -0.08150571 -0.5567457\n",
      "  -0.13884077  0.48396982 -0.15483233  0.1998452  -0.38052318  0.00574983]\n",
      " [ 0.00872382  0.35764006  0.13874704 -0.12783954 -0.1167231  -0.19682823\n",
      "  -0.17800632  0.51087901 -0.41612538  0.01987502 -0.0450581  -0.33373166\n",
      "   0.58628726  0.12455089 -0.23797826  0.49024353  0.54883338 -0.66267289]\n",
      " [ 0.54621789 -0.07752257  0.14840899  0.19012808 -0.38929394  0.95476906\n",
      "  -0.45997428 -1.02083753  0.49979735  0.29022955 -0.29442141 -0.24916741\n",
      "  -0.16947429 -0.15667383 -0.18956186 -0.48323016  0.0606541  -0.45262353]\n",
      " [-0.07427777  0.34240792 -0.2169319   0.53460816  0.29579757  0.77770849\n",
      "  -0.64685884 -0.20586614 -0.11026817 -0.52111597  0.35456302 -0.03616237\n",
      "   0.16358166 -0.77165439 -0.42980762  0.03108591  0.39504283 -0.73173153]\n",
      " [-0.33649421  0.34521017  0.17454043  0.45121552 -0.73560799 -0.15861332\n",
      "   0.06755126 -0.06979758 -0.16089523 -0.27075165  0.48885161 -0.3172311\n",
      "  -0.21293268 -0.41609911 -0.17235744  0.2343868  -0.29896389  0.13950116]\n",
      " [ 0.56382404 -0.31351601  0.03476986 -0.67640551 -0.41952352 -0.75870662\n",
      "  -0.57601468 -0.24028477 -0.37126841 -0.23708039  0.13204989 -0.69814712\n",
      "   0.11347724 -0.04171792  0.02305949  0.58885037  0.341841   -0.44788885]\n",
      " [-1.04702039 -0.34517245  0.03836775  0.6913737   0.60274015 -0.51607785\n",
      "   0.26250845 -0.12451921  0.10670339 -0.00497831  0.0636187  -0.53363659\n",
      "   0.04924746 -0.17894752 -0.72488188  0.1492776   0.25357843  0.12214889]]\n",
      "Done\n",
      "[[ 0.80255312 -0.37145196  0.48755532 -0.33667181 -0.3780045  -0.45530105\n",
      "   0.32544903 -0.12509503 -0.06355813 -0.21995553  0.13963478 -0.23846263\n",
      "  -0.42214992  0.29852426  0.12025178  0.04304943 -0.74163176  0.25904209]\n",
      " [-0.27446148  0.07462879  0.1567137   0.2544249   0.26416653 -0.785098\n",
      "   1.12323999  0.53742711  0.44374511 -0.14910145 -0.08150571 -0.5567457\n",
      "  -0.13884077  0.48396982 -0.15483233  0.1998452  -0.38052318  0.00574983]\n",
      " [ 0.00872382  0.35764006  0.13874704 -0.12783954 -0.1167231  -0.19682823\n",
      "  -0.17800632  0.51087901 -0.41612538  0.01987502 -0.0450581  -0.33373166\n",
      "   0.58628726  0.12455089 -0.23797826  0.49024353  0.54883338 -0.66267289]\n",
      " [ 0.54621789 -0.07752257  0.14840899  0.19012808 -0.38929394  0.95476906\n",
      "  -0.45997428 -1.02083753  0.49979735  0.29022955 -0.29442141 -0.24916741\n",
      "  -0.16947429 -0.15667383 -0.18956186 -0.48323016  0.0606541  -0.45262353]\n",
      " [-0.07427777  0.34240792 -0.2169319   0.53460816  0.29579757  0.77770849\n",
      "  -0.64685884 -0.20586614 -0.11026817 -0.52111597  0.35456302 -0.03616237\n",
      "   0.16358166 -0.77165439 -0.42980762  0.03108591  0.39504283 -0.73173153]\n",
      " [-0.33649421  0.34521017  0.17454043  0.45121552 -0.73560799 -0.15861332\n",
      "   0.06755126 -0.06979758 -0.16089523 -0.27075165  0.48885161 -0.3172311\n",
      "  -0.21293268 -0.41609911 -0.17235744  0.2343868  -0.29896389  0.13950116]\n",
      " [ 0.56382404 -0.31351601  0.03476986 -0.67640551 -0.41952352 -0.75870662\n",
      "  -0.57601468 -0.24028477 -0.37126841 -0.23708039  0.13204989 -0.69814712\n",
      "   0.11347724 -0.04171792  0.02305949  0.58885037  0.341841   -0.44788885]\n",
      " [-1.04702039 -0.34517245  0.03836775  0.6913737   0.60274015 -0.51607785\n",
      "   0.26250845 -0.12451921  0.10670339 -0.00497831  0.0636187  -0.53363659\n",
      "   0.04924746 -0.17894752 -0.72488188  0.1492776   0.25357843  0.12214889]]\n",
      "Cross Entropy = 3.592596088477645\n",
      "     Accuracy = 21.999999999999996%\n",
      "Cross Entropy = 3.59259608847764\n",
      "     Accuracy = 21.999999999999996%\n"
     ]
    }
   ],
   "source": [
    "print(net.W[0])\n",
    "print(\"Done\")\n",
    "print(net2.W[0])\n",
    "nets = [net, net2]\n",
    "for n in nets:\n",
    "    CE = n.Evaluate(train[0], train[1])\n",
    "    accuracy = n.ClassificationAccuracy(train[0], train[1])\n",
    "    print('Cross Entropy = '+str(CE))\n",
    "    print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Training Set')\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print('Test Set')\n",
    "CE = net.Evaluate(test[0], test[1])\n",
    "accuracy = net.ClassificationAccuracy(test[0], test[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## You can also try using the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import Network\n",
    "# importlib.reload(Network)\n",
    "# net2 = Network.Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# net2.Learn(train[0], train[1], epochs=500, lrate=1.)\n",
    "# print('Training Set')\n",
    "# CE = net2.Evaluate(train[0], train[1])\n",
    "# accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "# print('Cross Entropy = '+str(CE))\n",
    "# print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "# print('Test Set')\n",
    "# CE = net2.Evaluate(test[0], test[1])\n",
    "# accuracy = net2.ClassificationAccuracy(test[0], test[1])\n",
    "# print('Cross Entropy = '+str(CE))\n",
    "# print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1D -> 1D (linear mapping)\n",
    "np.random.seed(846)\n",
    "n_input = 1\n",
    "n_output = 1\n",
    "slope = np.random.rand() - 0.5\n",
    "intercept = np.random.rand()*2. - 1.\n",
    "\n",
    "def myfunc(x):\n",
    "    return slope*x+intercept\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 200\n",
    "training_output = []\n",
    "training_input = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    training_input.append(np.array([x]))\n",
    "    training_output.append(np.array([t]))\n",
    "\n",
    "# Create a testing dataset\n",
    "n_samples = 50\n",
    "test_input = []\n",
    "test_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx] + np.random.normal(scale=0.1)\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    test_input.append(np.array([x]))\n",
    "    test_output.append(np.array([t]))\n",
    "\n",
    "# Create a perfect dataset\n",
    "n_samples = 100\n",
    "perfect_input = []\n",
    "perfect_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x)\n",
    "    perfect_input.append(np.array([x]))\n",
    "    perfect_output.append(np.array([t]))\n",
    "    \n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]\n",
    "perfect = [np.array(perfect_input), np.array(perfect_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = Network([1, 10, 1], type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate it before training\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# On training dataset\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('Training MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# On test dataset\n",
    "mse = net.Evaluate(test[0], test[1])\n",
    "print('Test MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model and the TRUE solution (since we know it)\n",
    "s = np.linspace(-1, 1, 200)\n",
    "y = net.FeedForward(np.array([s]).T)\n",
    "p = [myfunc(x) for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Plot the training data,\n",
    "# as well as out model and the true model\n",
    "plt.plot(s,y, 'r--')\n",
    "plt.plot(s,p, 'g:')\n",
    "plt.plot(training_input, training_output, 'bo')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "q = t.copy()\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "t[0] = 2\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
