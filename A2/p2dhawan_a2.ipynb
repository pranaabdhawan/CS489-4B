{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOU_a2 (v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q1: Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The logistic function may be written as:\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\sigma (z)}&= \\frac{1} {1 + e^{-z}} \\\\\n",
    "{\\sigma (z)}&= \\frac{e^{z}} {e^{z}+1}\n",
    "\\end{align}\n",
    "$$\n",
    "Now, using Quotient rule:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d \\sigma(z)}{dz} &= \\frac{e^{z}(e^{z}+1) - e^{z}.e^{z}}{(1+e^{z})^2} \\\\\n",
    "\\frac{d \\sigma(z)}{dz} &= \\frac{e^{z}}{(1+e^{z})^2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Now,\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\sigma(z)(1-\\sigma(z))} &= \\frac{e^{z}}{1+e^{z}}.\\frac{1 + e^{z} - e^{z}}{1 + e^{z}} \\\\\n",
    "{\\sigma(z)(1-\\sigma(z))} &= \\frac{e^{z}}{(1+e^{z})^2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore, LHS = RHS.\n",
    "Hence proved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To help you with $\\LaTeX$, and to show you my expectations, here is a sample taken from the lecture notes, taken from the 3rd and 4th page of the notes entitled \"Error Backpropagation\". It has nothing to do with the solution to this question, but just demonstrates some of the features of $\\LaTeX$. Notice how I include English statments to guide the reader through the derivation.\n",
    "\n",
    "<a target=_new href=\"http://detexify.kirelabs.org/classify.html\">This web page</a> is very handy for identifying $\\LaTeX$ symbols.\n",
    "\n",
    "---\n",
    "More generally, for $\\vec{x} \\in \\mathbb{R}^X$, $\\vec{h} \\in \\mathbb{R}^H$, and $\\vec{y} \\in \\mathbb{R}^Y$.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial \\alpha_i}\n",
    "  &= \\frac{d h_i}{d \\alpha_i} \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "  \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right] \\cdot\n",
    "  \\left[ \\frac{\\partial E}{\\partial \\beta_1} \\ \\cdots \\ \\frac{\\partial E}{\\partial \\beta_Y} \\right] \\\\\n",
    "  &= \\frac{d h_i}{d \\alpha_i}\n",
    "   \\left[ M_{1i} \\ \\cdots \\ M_{Yi} \\right]\n",
    "   \\left[ \\begin{array}{c}\n",
    "     \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "     \\vdots \\\\\n",
    "     \\frac{\\partial E}{\\partial \\beta_Y} \\end{array} \\right]\n",
    "\\end{align}\n",
    "$$\n",
    "Thus, for all elements,\n",
    "$$\n",
    "\\begin{align}\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\alpha_H}\n",
    "\\end{array} \\right] &=\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{d h_1}{d \\alpha_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{d h_H}{d \\alpha_H}\n",
    "\\end{array} \\right]\n",
    "\\odot\n",
    "\\left[ \\begin{array}{ccc}\n",
    "  M_{11} & \\cdots & M_{Y1} \\\\\n",
    "  \\vdots & \\ddots & \\vdots \\\\\n",
    "  M_{1H} & \\cdots & M_{YH}\n",
    "\\end{array} \\right]\n",
    "%\n",
    "\\left[ \\begin{array}{c}\n",
    "  \\frac{\\partial E}{\\partial \\beta_1} \\\\\n",
    "  \\vdots \\\\\n",
    "  \\frac{\\partial E}{\\partial \\beta_Y}\n",
    "\\end{array} \\right] \\\\\n",
    "%\n",
    "\\frac{\\partial E}{\\partial \\vec{\\alpha}} &=\n",
    "\\frac{d \\vec{h}}{d \\vec{\\alpha}} \\odot M^\\mathrm{T}\n",
    "\\frac{\\partial E}{\\partial \\vec{\\beta}}\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, lets find the gradient of the softmax function, using Quotient rule:   \n",
    "If m = k:  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y_{k}}{\\partial z_{m}} &= \\frac{({e^{z_{k}}.\\sum_{j=1}^{K}{e^{z_{j}}}}) - e^{z_{m}}.e^{z_{k}}}{(\\sum_{j=1}^{K}{e^{z_{j}}})^{2}} \\\\\n",
    "\\frac{\\partial y_{k}}{\\partial z_{m}} &= \\frac{e^{z_k}.(\\sum_{j=1}^{K}{e^{z_{j}} - e^{z_{m}}})}{(\\sum_{j=1}^{K}{e^{z_{j}}}).(\\sum_{j=1}^{K}{e^{z_{j}}})}\\\\\n",
    "\\frac{\\partial y_{k}}{\\partial z_{m}} &= y_{k}.(1 - y_{m}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "If m!=k, applying the quotient rule simplifies to:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y_{k}}{\\partial z_{m}} &= \\frac{- e^{z_k}.e^{z_{m}}}{(\\sum_{j=1}^{K}{e^{z_{j}}})^{2}} \\\\\n",
    "\\frac{\\partial y_{k}}{\\partial z_{m}} &= - y_{k}y_{m} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, let's compute the derivative of the categorical cross entropy and apply chain rule:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_{j}} &= -\\sum_{k=1}^{K}{t_{k}}.\\frac{\\partial log(y_{k})}{\\partial z_{j}}\\\\\n",
    "&= -\\sum_{k=1}^{K}{\\frac{t_{k}}{y_{k}}}.\\frac{\\partial y_{k}}{\\partial z_{j}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Substituting the derivative of the Softmax function from above:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_{j}} &= (\\sum_{k \\neq j}^{K}{(y_{k}.y_{j})\\frac{t_{k}}{y_{k}}}) - t_{j}(1-y_{j})\\\\\n",
    "&= -t_{j} + y_{j}((\\sum_{k \\neq j}^{K}{t_{k}) + t_{j}})\n",
    "\\end{align}\n",
    "$$\n",
    "If we assume the given problem is multi-class single-label (a fair assumption to make since we are using softmax), then:  \n",
    "$$\n",
    "\\begin{align}\n",
    "(\\sum_{k \\neq j}^{K}{t_{k}) + t_{j}} &= 1\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore our result is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_{j}} &= y_{j} - t_{j}\n",
    "\\end{align}\n",
    "$$\n",
    "Hence, proved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q3: Top-Layer Error Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Q4: Implementing Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Supplied Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "metadata": {
    "code_folding": [
     16,
     35
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Supplied functions\n",
    "\n",
    "def NSamples(x):\n",
    "    '''\n",
    "        n = NSamples(x)\n",
    "        \n",
    "        Returns the number of samples in a batch of inputs.\n",
    "        \n",
    "        Input:\n",
    "         x   is a 2D array\n",
    "        \n",
    "        Output:\n",
    "         n   is an integer\n",
    "    '''\n",
    "    return len(x)\n",
    "\n",
    "def Shuffle(inputs, targets):\n",
    "    '''\n",
    "        s_inputs, s_targets = Shuffle(inputs, targets)\n",
    "        \n",
    "        Randomly shuffles the dataset.\n",
    "        \n",
    "        Inputs:\n",
    "         inputs     array of inputs\n",
    "         targets    array of corresponding targets\n",
    "         \n",
    "        Outputs:\n",
    "         s_inputs   shuffled array of inputs\n",
    "         s_targets  corresponding shuffled array of targets\n",
    "    '''\n",
    "    data = list(zip(inputs,targets))\n",
    "    np.random.shuffle(data)\n",
    "    s_inputs, s_targets = zip(*data)\n",
    "    return np.array(s_inputs), np.array(s_targets)\n",
    "\n",
    "def Logistic(z):\n",
    "    '''\n",
    "        y = Logistic(z)\n",
    "\n",
    "        Applies the logistic function to each element in z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return 1. / (1 + np.exp(-z) )\n",
    "\n",
    "def Logistic_p(h):\n",
    "    '''\n",
    "        yp = Logistic_p(h)\n",
    "        \n",
    "        Returns the slope of the logistic function at z when h = Logistic(z).\n",
    "        Note the h is the input, NOT z.\n",
    "    '''\n",
    "    return h*(1.-h)\n",
    "\n",
    "def Identity(z):\n",
    "    '''\n",
    "        y = Identity(z)\n",
    "\n",
    "        Does nothing... simply returns z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return z\n",
    "\n",
    "def Identity_p(h):\n",
    "    '''\n",
    "        yp = Identity_p(h)\n",
    "        \n",
    "        Returns the slope of the identity function h.\n",
    "    '''\n",
    "    return np.ones_like(h)\n",
    "\n",
    "def OneHot(z):\n",
    "    '''\n",
    "        y = OneHot(z)\n",
    "\n",
    "        Applies the one-hot function to the vectors in z.\n",
    "        Example:\n",
    "          OneHot([[0.9, 0.1], [-0.5, 0.1]])\n",
    "          returns np.array([[1,0],[0,1]])\n",
    "\n",
    "        Input:\n",
    "         z    is a 2D array of samples\n",
    "\n",
    "        Output:\n",
    "         y    is an array the same shape as z\n",
    "    '''\n",
    "    y = []\n",
    "    # Locate the max of each row\n",
    "    for zz in z:\n",
    "        idx = np.argmax(zz)\n",
    "        b = np.zeros_like(zz)\n",
    "        b[idx] = 1.\n",
    "        y.append(b)\n",
    "    y = np.array(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def __init__(self, n_nodes, act='logistic'):\n",
    "        '''\n",
    "            lyr = Layer(n_nodes, act='logistic')\n",
    "            \n",
    "            Creates a layer object.\n",
    "            \n",
    "            Inputs:\n",
    "             n_nodes  the number of nodes in the layer\n",
    "             act      specifies the activation function\n",
    "                      Use 'logistic' or 'identity'\n",
    "        '''\n",
    "        self.N = n_nodes  # number of nodes in this layer\n",
    "        self.h = []       # node activities\n",
    "        self.b = np.zeros(self.N)  # biases\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigma = Logistic\n",
    "        self.sigma_p = (lambda : Logistic_p(self.h))\n",
    "        if act=='identity':\n",
    "            self.sigma = Identity\n",
    "            self.sigma_p = (lambda : Identity_p(self.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {
    "code_folding": [
     2,
     65,
     81
    ]
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self, sizes, type='classifier'):\n",
    "        '''\n",
    "            net = Network(sizes, type='classifier')\n",
    "\n",
    "            Creates a Network and saves it in the variable 'net'.\n",
    "\n",
    "            Inputs:\n",
    "              sizes is a list of integers specifying the number\n",
    "                  of nodes in each layer\n",
    "                  eg. [5, 20, 3] will create a 3-layer network\n",
    "                      with 5 input, 20 hidden, and 3 output nodes\n",
    "              type can be either 'classifier' or 'regression', and\n",
    "                  sets the activation function on the output layer,\n",
    "                  as well as the loss function.\n",
    "                  'classifier': logistic, cross entropy\n",
    "                  'regression': linear, mean squared error\n",
    "        '''\n",
    "        self.n_layers = len(sizes)\n",
    "        self.lyr = []    # a list of Layers\n",
    "        self.W = []      # Weight matrices, indexed by the layer below it\n",
    "        \n",
    "        # Two common types of networks\n",
    "        # The member variable self.Loss refers to one of the implemented\n",
    "        # loss functions: MSE, or CrossEntropy.\n",
    "        # Call it using self.Loss(t)\n",
    "        if type=='classifier':\n",
    "            self.classifier = True\n",
    "            self.Loss = self.CrossEntropy\n",
    "            activation = 'logistic'\n",
    "        else:\n",
    "            self.classifier = False\n",
    "            self.Loss = self.MSE\n",
    "            activation = 'identity'\n",
    "\n",
    "        # Create and add Layers (using logistic for hidden layers)\n",
    "        for n in sizes[:-1]:\n",
    "            self.lyr.append( Layer(n) )\n",
    "   \n",
    "        # For the top layer, we use the appropriate activtaion function\n",
    "        self.lyr.append( Layer(sizes[-1], act=activation) )\n",
    "    \n",
    "        # Randomly initialize weight matrices\n",
    "        for idx in range(self.n_layers-1):\n",
    "            m = self.lyr[idx].N\n",
    "            n = self.lyr[idx+1].N\n",
    "            temp = np.random.normal(size=[m,n])/np.sqrt(m)\n",
    "            self.W.append(temp)\n",
    "\n",
    "\n",
    "    def FeedForward(self, x):\n",
    "        '''\n",
    "            y = net.FeedForward(x)\n",
    "\n",
    "            Runs the network forward, starting with x as input.\n",
    "            Returns the activity of the output layer.\n",
    "        '''\n",
    "        x = np.array(x)  # Convert input to array, in case it's not\n",
    "        \n",
    "        # TODO: Check the code for biases.\n",
    "        \n",
    "        # layer.h will be of the shape : [num_samples, num_nodes]\n",
    "        for i in range(len(self.lyr)):\n",
    "            if i == 0:\n",
    "                self.lyr[i].h = x\n",
    "            else:\n",
    "                last_layer_output = self.lyr[i-1].h\n",
    "                linear = np.dot(last_layer_output, self.W[i-1])\n",
    "                linear += np.outer(np.ones(len(x)), self.lyr[i].b)\n",
    "                self.lyr[i].h = self.lyr[i].sigma(linear)\n",
    "                \n",
    "        \n",
    "        return self.lyr[-1].h\n",
    "\n",
    "    \n",
    "    def Evaluate(self, inputs, targets):\n",
    "        '''\n",
    "            E = net.Evaluate(data)\n",
    "\n",
    "            Computes the average loss over the supplied dataset.\n",
    "\n",
    "            Inputs\n",
    "             inputs  is an array of inputs\n",
    "             targets is a list of corresponding targets\n",
    "\n",
    "            Outputs\n",
    "             E is a scalar, the average loss\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        return self.Loss(targets)\n",
    "\n",
    "    def ClassificationAccuracy(self, inputs, targets):\n",
    "        '''\n",
    "            a = net.ClassificationAccuracy(data)\n",
    "            \n",
    "            Returns the fraction (between 0 and 1) of correct one-hot classifications\n",
    "            in the dataset.\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        yb = OneHot(y)\n",
    "        n_incorrect = np.sum(yb!=targets) / 2.\n",
    "        return 1. - float(n_incorrect) / NSamples(inputs)\n",
    "\n",
    "    \n",
    "    def CrossEntropy(self, t):\n",
    "        '''\n",
    "            E = net.CrossEntropy(t)\n",
    "\n",
    "            Evaluates the mean cross entropy loss between t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        #===== YOUR CODE HERE =====\n",
    "        output_activity = self.lyr[-1].h # shape = (num_samples, num_outputs)\n",
    "        #shape of t: (num_samples, num_outputs)\n",
    "        \n",
    "        # Equation for Cross Entropy (or negative log likelihood for observing the data given the parameters)\n",
    "        # - (ylog(y) + (1-y)log(1-p))\n",
    "        # Take the mean\n",
    "        # TODO: refactor this.\n",
    "        result = 0\n",
    "        for i in range(len(t)):\n",
    "            for j in range(t.shape[1]):\n",
    "                if t[i][j] == 1:\n",
    "                        result += np.log(output_activity[i][j])\n",
    "                else:\n",
    "                        result += np.log(1.-output_activity[i][j])\n",
    "        return (-1*result)/len(t)\n",
    "\n",
    "    \n",
    "    def MSE(self, t):\n",
    "        '''\n",
    "            E = net.MSE(t)\n",
    "\n",
    "            Evaluates the MSE loss function using t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        output_activity = self.lyr[-1].h\n",
    "        error = np.mean(np.square(output_activity - t))\n",
    "        \n",
    "        return error\n",
    "\n",
    "    \n",
    "    def BackProp(self, t, lrate=0.05):\n",
    "        '''\n",
    "            net.BackProp(targets, lrate=0.05)\n",
    "            \n",
    "            Given the current network state and targets t, updates the connection\n",
    "            weights and biases using the backpropagation algorithm.\n",
    "            \n",
    "            Inputs:\n",
    "             t      an array of targets (number of samples must match the\n",
    "                    network's output)\n",
    "             lrate  learning rate\n",
    "        '''\n",
    "        t = np.array(t)  # convert t to an array, in case it's not\n",
    "        \n",
    "        # We are following (i-1) -> i at index i\n",
    "        for i in range(len(self.lyr)-1,0,-1):\n",
    "            if i == len(self.lyr)-1:\n",
    "                de_dz = (self.lyr[-1].h - t).T # The gradient w.r.t. to the outermost layer: Y x P\n",
    "            else:\n",
    "                dh_dz = self.lyr[i].sigma_p().T\n",
    "#                 dh_dz = Logistic_p(self.lyr[i].h).T # This is H x P\n",
    "                # W[i] is H x Y, de_dz is Y x P\n",
    "                de_dz = np.multiply(dh_dz, np.dot(self.W[i], de_dz)) # This will be H x P\n",
    "                \n",
    "            # lyr[i-1].h is P x H, de_dz is Y x P or H(i+1) x P\n",
    "            de_dw = np.dot(de_dz, self.lyr[i-1].h).T # To simplify : This will be H x Y\n",
    "            de_dw /= t.shape[0]\n",
    "            \n",
    "            de_db = np.sum(de_dz, axis=1)/t.shape[0] # Length : Y\n",
    "            self.lyr[i].b -= de_db\n",
    "            \n",
    "            self.W[i-1] -= lrate*de_dw\n",
    "\n",
    "        \n",
    "\n",
    "    def Learn(self, inputs, targets, lrate=0.05, epochs=1):\n",
    "        '''\n",
    "            Network.Learn(inputs, targets, lrate=0.05, epochs=1)\n",
    "\n",
    "            Run through the dataset 'epochs' number of times, incrementing the\n",
    "            network weights for each training sample. For each epoch, it\n",
    "            shuffles the order of the samples.\n",
    "\n",
    "            Inputs:\n",
    "              inputs  is an array of input samples\n",
    "              targets is a corresponding array of targets\n",
    "              lrate   is the learning rate (try 0.001 to 0.5)\n",
    "              epochs  is the number of times to go through the training data\n",
    "        '''\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            train_inputs, train_targets = Shuffle(inputs, targets)\n",
    "            y = self.FeedForward(train_inputs)\n",
    "            self.BackProp(train_targets, lrate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 5 Classes in 8-Dimensional Space\n",
    "np.random.seed(15)\n",
    "noise = 0.1\n",
    "InputClasses = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "OutputClasses = np.array([[1,0,0,0,0],\n",
    "                          [0,1,0,0,0],\n",
    "                          [0,0,1,0,0],\n",
    "                          [0,0,0,1,0],\n",
    "                          [0,0,0,0,1]], dtype=float)\n",
    "n_input = np.shape(InputClasses)[1]\n",
    "n_output = np.shape(OutputClasses)[1]\n",
    "n_classes = np.shape(InputClasses)[0]\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 100\n",
    "training_output = []\n",
    "training_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    training_input.append(x)\n",
    "    training_output.append(t)\n",
    "\n",
    "# Create a test dataset\n",
    "n_samples = 100\n",
    "test_output = []\n",
    "test_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    test_input.append(x)\n",
    "    test_output.append(t)\n",
    "\n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Learn(network_param, inputs, targets, lrate=0.05, epochs=1):\n",
    "#     '''\n",
    "#         Network.Learn(inputs, targets, lrate=0.05, epochs=1)\n",
    "\n",
    "#         Run through the dataset 'epochs' number of times, incrementing the\n",
    "#         network weights for each training sample. For each epoch, it\n",
    "#         shuffles the order of the samples.\n",
    "\n",
    "#         Inputs:\n",
    "#           inputs  is an array of input samples\n",
    "#           targets is a corresponding array of targets\n",
    "#           lrate   is the learning rate (try 0.001 to 0.5)\n",
    "#           epochs  is the number of times to go through the training data\n",
    "#     '''\n",
    "\n",
    "#     for i in range(epochs):\n",
    "#         train_inputs, train_targets = Shuffle(inputs, targets)\n",
    "#         y = network_param.FeedForward(train_inputs)\n",
    "#         network_param.BackProp(train_targets, lrate)\n",
    "#         print(\"Training Loss %.5f\" % network_param.Loss(train_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1208,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1209,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 3.617051325333447\n",
      "     Accuracy = 26.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1210,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=500, lrate=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "Cross Entropy = 0.017728922146593974\n",
      "     Accuracy = 100.0%\n",
      "Test Set\n",
      "Cross Entropy = 0.01903443433918009\n",
      "     Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Training Set')\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "print('Test Set')\n",
    "CE = net.Evaluate(test[0], test[1])\n",
    "accuracy = net.ClassificationAccuracy(test[0], test[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## You can also try using the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1156,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import Network_solutions as sol\n",
    "# net2 = sol.Network([n_input, 18, n_output], type='classifier')\n",
    "# from copy import deepcopy\n",
    "# net2.W = copy.deepcopy(net.W)\n",
    "# accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "# print('Cross Entropy = '+str(CE))\n",
    "# print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn(net2, train[0], train[1], epochs=500, lrate=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss 0.01773\n",
      "Training Loss 0.01769\n",
      "Training Loss 0.01764\n",
      "Training Loss 0.01760\n",
      "Training Loss 0.01756\n",
      "Training Loss 0.01752\n",
      "Training Loss 0.01747\n",
      "Training Loss 0.01743\n",
      "Training Loss 0.01739\n",
      "Training Loss 0.01735\n",
      "Training Loss 0.01731\n",
      "Training Loss 0.01727\n",
      "Training Loss 0.01722\n",
      "Training Loss 0.01718\n",
      "Training Loss 0.01714\n",
      "Training Loss 0.01710\n",
      "Training Loss 0.01706\n",
      "Training Loss 0.01702\n",
      "Training Loss 0.01698\n",
      "Training Loss 0.01694\n",
      "Training Loss 0.01690\n",
      "Training Loss 0.01686\n",
      "Training Loss 0.01683\n",
      "Training Loss 0.01679\n",
      "Training Loss 0.01675\n",
      "Training Loss 0.01671\n",
      "Training Loss 0.01667\n",
      "Training Loss 0.01663\n",
      "Training Loss 0.01659\n",
      "Training Loss 0.01656\n",
      "Training Loss 0.01652\n",
      "Training Loss 0.01648\n",
      "Training Loss 0.01644\n",
      "Training Loss 0.01641\n",
      "Training Loss 0.01637\n",
      "Training Loss 0.01633\n",
      "Training Loss 0.01629\n",
      "Training Loss 0.01626\n",
      "Training Loss 0.01622\n",
      "Training Loss 0.01618\n",
      "Training Loss 0.01615\n",
      "Training Loss 0.01611\n",
      "Training Loss 0.01608\n",
      "Training Loss 0.01604\n",
      "Training Loss 0.01601\n",
      "Training Loss 0.01597\n",
      "Training Loss 0.01593\n",
      "Training Loss 0.01590\n",
      "Training Loss 0.01586\n",
      "Training Loss 0.01583\n",
      "Training Loss 0.01579\n",
      "Training Loss 0.01576\n",
      "Training Loss 0.01573\n",
      "Training Loss 0.01569\n",
      "Training Loss 0.01566\n",
      "Training Loss 0.01562\n",
      "Training Loss 0.01559\n",
      "Training Loss 0.01556\n",
      "Training Loss 0.01552\n",
      "Training Loss 0.01549\n",
      "Training Loss 0.01546\n",
      "Training Loss 0.01542\n",
      "Training Loss 0.01539\n",
      "Training Loss 0.01536\n",
      "Training Loss 0.01532\n",
      "Training Loss 0.01529\n",
      "Training Loss 0.01526\n",
      "Training Loss 0.01523\n",
      "Training Loss 0.01519\n",
      "Training Loss 0.01516\n",
      "Training Loss 0.01513\n",
      "Training Loss 0.01510\n",
      "Training Loss 0.01507\n",
      "Training Loss 0.01503\n",
      "Training Loss 0.01500\n",
      "Training Loss 0.01497\n",
      "Training Loss 0.01494\n",
      "Training Loss 0.01491\n",
      "Training Loss 0.01488\n",
      "Training Loss 0.01485\n",
      "Training Loss 0.01482\n",
      "Training Loss 0.01479\n",
      "Training Loss 0.01476\n",
      "Training Loss 0.01472\n",
      "Training Loss 0.01469\n",
      "Training Loss 0.01466\n",
      "Training Loss 0.01463\n",
      "Training Loss 0.01460\n",
      "Training Loss 0.01457\n",
      "Training Loss 0.01454\n",
      "Training Loss 0.01452\n",
      "Training Loss 0.01449\n",
      "Training Loss 0.01446\n",
      "Training Loss 0.01443\n",
      "Training Loss 0.01440\n",
      "Training Loss 0.01437\n",
      "Training Loss 0.01434\n",
      "Training Loss 0.01431\n",
      "Training Loss 0.01428\n",
      "Training Loss 0.01425\n",
      "Training Loss 0.01423\n",
      "Training Loss 0.01420\n",
      "Training Loss 0.01417\n",
      "Training Loss 0.01414\n",
      "Training Loss 0.01411\n",
      "Training Loss 0.01408\n",
      "Training Loss 0.01406\n",
      "Training Loss 0.01403\n",
      "Training Loss 0.01400\n",
      "Training Loss 0.01397\n",
      "Training Loss 0.01395\n",
      "Training Loss 0.01392\n",
      "Training Loss 0.01389\n",
      "Training Loss 0.01387\n",
      "Training Loss 0.01384\n",
      "Training Loss 0.01381\n",
      "Training Loss 0.01378\n",
      "Training Loss 0.01376\n",
      "Training Loss 0.01373\n",
      "Training Loss 0.01370\n",
      "Training Loss 0.01368\n",
      "Training Loss 0.01365\n",
      "Training Loss 0.01363\n",
      "Training Loss 0.01360\n",
      "Training Loss 0.01357\n",
      "Training Loss 0.01355\n",
      "Training Loss 0.01352\n",
      "Training Loss 0.01350\n",
      "Training Loss 0.01347\n",
      "Training Loss 0.01345\n",
      "Training Loss 0.01342\n",
      "Training Loss 0.01339\n",
      "Training Loss 0.01337\n",
      "Training Loss 0.01334\n",
      "Training Loss 0.01332\n",
      "Training Loss 0.01329\n",
      "Training Loss 0.01327\n",
      "Training Loss 0.01324\n",
      "Training Loss 0.01322\n",
      "Training Loss 0.01319\n",
      "Training Loss 0.01317\n",
      "Training Loss 0.01315\n",
      "Training Loss 0.01312\n",
      "Training Loss 0.01310\n",
      "Training Loss 0.01307\n",
      "Training Loss 0.01305\n",
      "Training Loss 0.01302\n",
      "Training Loss 0.01300\n",
      "Training Loss 0.01298\n",
      "Training Loss 0.01295\n",
      "Training Loss 0.01293\n",
      "Training Loss 0.01291\n",
      "Training Loss 0.01288\n",
      "Training Loss 0.01286\n",
      "Training Loss 0.01283\n",
      "Training Loss 0.01281\n",
      "Training Loss 0.01279\n",
      "Training Loss 0.01277\n",
      "Training Loss 0.01274\n",
      "Training Loss 0.01272\n",
      "Training Loss 0.01270\n",
      "Training Loss 0.01267\n",
      "Training Loss 0.01265\n",
      "Training Loss 0.01263\n",
      "Training Loss 0.01261\n",
      "Training Loss 0.01258\n",
      "Training Loss 0.01256\n",
      "Training Loss 0.01254\n",
      "Training Loss 0.01252\n",
      "Training Loss 0.01249\n",
      "Training Loss 0.01247\n",
      "Training Loss 0.01245\n",
      "Training Loss 0.01243\n",
      "Training Loss 0.01241\n",
      "Training Loss 0.01238\n",
      "Training Loss 0.01236\n",
      "Training Loss 0.01234\n",
      "Training Loss 0.01232\n",
      "Training Loss 0.01230\n",
      "Training Loss 0.01228\n",
      "Training Loss 0.01225\n",
      "Training Loss 0.01223\n",
      "Training Loss 0.01221\n",
      "Training Loss 0.01219\n",
      "Training Loss 0.01217\n",
      "Training Loss 0.01215\n",
      "Training Loss 0.01213\n",
      "Training Loss 0.01211\n",
      "Training Loss 0.01209\n",
      "Training Loss 0.01206\n",
      "Training Loss 0.01204\n",
      "Training Loss 0.01202\n",
      "Training Loss 0.01200\n",
      "Training Loss 0.01198\n",
      "Training Loss 0.01196\n",
      "Training Loss 0.01194\n",
      "Training Loss 0.01192\n",
      "Training Loss 0.01190\n",
      "Training Loss 0.01188\n",
      "Training Loss 0.01186\n",
      "Training Loss 0.01184\n",
      "Training Loss 0.01182\n",
      "Training Loss 0.01180\n",
      "Training Loss 0.01178\n",
      "Training Loss 0.01176\n",
      "Training Loss 0.01174\n",
      "Training Loss 0.01172\n",
      "Training Loss 0.01170\n",
      "Training Loss 0.01168\n",
      "Training Loss 0.01166\n",
      "Training Loss 0.01164\n",
      "Training Loss 0.01162\n",
      "Training Loss 0.01160\n",
      "Training Loss 0.01159\n",
      "Training Loss 0.01157\n",
      "Training Loss 0.01155\n",
      "Training Loss 0.01153\n",
      "Training Loss 0.01151\n",
      "Training Loss 0.01149\n",
      "Training Loss 0.01147\n",
      "Training Loss 0.01145\n",
      "Training Loss 0.01143\n",
      "Training Loss 0.01141\n",
      "Training Loss 0.01140\n",
      "Training Loss 0.01138\n",
      "Training Loss 0.01136\n",
      "Training Loss 0.01134\n",
      "Training Loss 0.01132\n",
      "Training Loss 0.01130\n",
      "Training Loss 0.01129\n",
      "Training Loss 0.01127\n",
      "Training Loss 0.01125\n",
      "Training Loss 0.01123\n",
      "Training Loss 0.01121\n",
      "Training Loss 0.01119\n",
      "Training Loss 0.01118\n",
      "Training Loss 0.01116\n",
      "Training Loss 0.01114\n",
      "Training Loss 0.01112\n",
      "Training Loss 0.01111\n",
      "Training Loss 0.01109\n",
      "Training Loss 0.01107\n",
      "Training Loss 0.01105\n",
      "Training Loss 0.01103\n",
      "Training Loss 0.01102\n",
      "Training Loss 0.01100\n",
      "Training Loss 0.01098\n",
      "Training Loss 0.01097\n",
      "Training Loss 0.01095\n",
      "Training Loss 0.01093\n",
      "Training Loss 0.01091\n",
      "Training Loss 0.01090\n",
      "Training Loss 0.01088\n",
      "Training Loss 0.01086\n",
      "Training Loss 0.01085\n",
      "Training Loss 0.01083\n",
      "Training Loss 0.01081\n",
      "Training Loss 0.01079\n",
      "Training Loss 0.01078\n",
      "Training Loss 0.01076\n",
      "Training Loss 0.01074\n",
      "Training Loss 0.01073\n",
      "Training Loss 0.01071\n",
      "Training Loss 0.01070\n",
      "Training Loss 0.01068\n",
      "Training Loss 0.01066\n",
      "Training Loss 0.01065\n",
      "Training Loss 0.01063\n",
      "Training Loss 0.01061\n",
      "Training Loss 0.01060\n",
      "Training Loss 0.01058\n",
      "Training Loss 0.01056\n",
      "Training Loss 0.01055\n",
      "Training Loss 0.01053\n",
      "Training Loss 0.01052\n",
      "Training Loss 0.01050\n",
      "Training Loss 0.01048\n",
      "Training Loss 0.01047\n",
      "Training Loss 0.01045\n",
      "Training Loss 0.01044\n",
      "Training Loss 0.01042\n",
      "Training Loss 0.01041\n",
      "Training Loss 0.01039\n",
      "Training Loss 0.01037\n",
      "Training Loss 0.01036\n",
      "Training Loss 0.01034\n",
      "Training Loss 0.01033\n",
      "Training Loss 0.01031\n",
      "Training Loss 0.01030\n",
      "Training Loss 0.01028\n",
      "Training Loss 0.01027\n",
      "Training Loss 0.01025\n",
      "Training Loss 0.01024\n",
      "Training Loss 0.01022\n",
      "Training Loss 0.01021\n",
      "Training Loss 0.01019\n",
      "Training Loss 0.01018\n",
      "Training Loss 0.01016\n",
      "Training Loss 0.01015\n",
      "Training Loss 0.01013\n",
      "Training Loss 0.01012\n",
      "Training Loss 0.01010\n",
      "Training Loss 0.01009\n",
      "Training Loss 0.01007\n",
      "Training Loss 0.01006\n",
      "Training Loss 0.01004\n",
      "Training Loss 0.01003\n",
      "Training Loss 0.01001\n",
      "Training Loss 0.01000\n",
      "Training Loss 0.00998\n",
      "Training Loss 0.00997\n",
      "Training Loss 0.00996\n",
      "Training Loss 0.00994\n",
      "Training Loss 0.00993\n",
      "Training Loss 0.00991\n",
      "Training Loss 0.00990\n",
      "Training Loss 0.00988\n",
      "Training Loss 0.00987\n",
      "Training Loss 0.00986\n",
      "Training Loss 0.00984\n",
      "Training Loss 0.00983\n",
      "Training Loss 0.00981\n",
      "Training Loss 0.00980\n",
      "Training Loss 0.00979\n",
      "Training Loss 0.00977\n",
      "Training Loss 0.00976\n",
      "Training Loss 0.00974\n",
      "Training Loss 0.00973\n",
      "Training Loss 0.00972\n",
      "Training Loss 0.00970\n",
      "Training Loss 0.00969\n",
      "Training Loss 0.00968\n",
      "Training Loss 0.00966\n",
      "Training Loss 0.00965\n",
      "Training Loss 0.00964\n",
      "Training Loss 0.00962\n",
      "Training Loss 0.00961\n",
      "Training Loss 0.00959\n",
      "Training Loss 0.00958\n",
      "Training Loss 0.00957\n",
      "Training Loss 0.00955\n",
      "Training Loss 0.00954\n",
      "Training Loss 0.00953\n",
      "Training Loss 0.00952\n",
      "Training Loss 0.00950\n",
      "Training Loss 0.00949\n",
      "Training Loss 0.00948\n",
      "Training Loss 0.00946\n",
      "Training Loss 0.00945\n",
      "Training Loss 0.00944\n",
      "Training Loss 0.00942\n",
      "Training Loss 0.00941\n",
      "Training Loss 0.00940\n",
      "Training Loss 0.00938\n",
      "Training Loss 0.00937\n",
      "Training Loss 0.00936\n",
      "Training Loss 0.00935\n",
      "Training Loss 0.00933\n",
      "Training Loss 0.00932\n",
      "Training Loss 0.00931\n",
      "Training Loss 0.00930\n",
      "Training Loss 0.00928\n",
      "Training Loss 0.00927\n",
      "Training Loss 0.00926\n",
      "Training Loss 0.00925\n",
      "Training Loss 0.00923\n",
      "Training Loss 0.00922\n",
      "Training Loss 0.00921\n",
      "Training Loss 0.00920\n",
      "Training Loss 0.00918\n",
      "Training Loss 0.00917\n",
      "Training Loss 0.00916\n",
      "Training Loss 0.00915\n",
      "Training Loss 0.00913\n",
      "Training Loss 0.00912\n",
      "Training Loss 0.00911\n",
      "Training Loss 0.00910\n",
      "Training Loss 0.00909\n",
      "Training Loss 0.00907\n",
      "Training Loss 0.00906\n",
      "Training Loss 0.00905\n",
      "Training Loss 0.00904\n",
      "Training Loss 0.00903\n",
      "Training Loss 0.00901\n",
      "Training Loss 0.00900\n",
      "Training Loss 0.00899\n",
      "Training Loss 0.00898\n",
      "Training Loss 0.00897\n",
      "Training Loss 0.00895\n",
      "Training Loss 0.00894\n",
      "Training Loss 0.00893\n",
      "Training Loss 0.00892\n",
      "Training Loss 0.00891\n",
      "Training Loss 0.00890\n",
      "Training Loss 0.00888\n",
      "Training Loss 0.00887\n",
      "Training Loss 0.00886\n",
      "Training Loss 0.00885\n",
      "Training Loss 0.00884\n",
      "Training Loss 0.00883\n",
      "Training Loss 0.00882\n",
      "Training Loss 0.00880\n",
      "Training Loss 0.00879\n",
      "Training Loss 0.00878\n",
      "Training Loss 0.00877\n",
      "Training Loss 0.00876\n",
      "Training Loss 0.00875\n",
      "Training Loss 0.00874\n",
      "Training Loss 0.00873\n",
      "Training Loss 0.00871\n",
      "Training Loss 0.00870\n",
      "Training Loss 0.00869\n",
      "Training Loss 0.00868\n",
      "Training Loss 0.00867\n",
      "Training Loss 0.00866\n",
      "Training Loss 0.00865\n",
      "Training Loss 0.00864\n",
      "Training Loss 0.00863\n",
      "Training Loss 0.00862\n",
      "Training Loss 0.00860\n",
      "Training Loss 0.00859\n",
      "Training Loss 0.00858\n",
      "Training Loss 0.00857\n",
      "Training Loss 0.00856\n",
      "Training Loss 0.00855\n",
      "Training Loss 0.00854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss 0.00853\n",
      "Training Loss 0.00852\n",
      "Training Loss 0.00851\n",
      "Training Loss 0.00850\n",
      "Training Loss 0.00849\n",
      "Training Loss 0.00848\n",
      "Training Loss 0.00846\n",
      "Training Loss 0.00845\n",
      "Training Loss 0.00844\n",
      "Training Loss 0.00843\n",
      "Training Loss 0.00842\n",
      "Training Loss 0.00841\n",
      "Training Loss 0.00840\n",
      "Training Loss 0.00839\n",
      "Training Loss 0.00838\n",
      "Training Loss 0.00837\n",
      "Training Loss 0.00836\n",
      "Training Loss 0.00835\n",
      "Training Loss 0.00834\n",
      "Training Loss 0.00833\n",
      "Training Loss 0.00832\n",
      "Training Loss 0.00831\n",
      "Training Loss 0.00830\n",
      "Training Loss 0.00829\n",
      "Training Loss 0.00828\n",
      "Training Loss 0.00827\n",
      "Training Loss 0.00826\n",
      "Training Loss 0.00825\n",
      "Training Loss 0.00824\n",
      "Training Loss 0.00823\n",
      "Training Loss 0.00822\n",
      "Training Loss 0.00821\n",
      "Training Loss 0.00820\n",
      "Training Loss 0.00819\n",
      "Training Loss 0.00818\n",
      "Training Loss 0.00817\n",
      "Training Loss 0.00816\n",
      "Training Loss 0.00815\n",
      "Training Loss 0.00814\n",
      "Training Loss 0.00813\n",
      "Training Loss 0.00812\n",
      "Training Loss 0.00811\n",
      "Training Loss 0.00810\n",
      "Training Loss 0.00809\n",
      "Training Loss 0.00808\n",
      "Training Loss 0.00807\n",
      "Training Loss 0.00806\n",
      "Training Loss 0.00805\n",
      "Training Loss 0.00804\n",
      "Training Loss 0.00803\n",
      "Training Loss 0.00802\n",
      "Training Loss 0.00801\n",
      "Training Loss 0.00800\n",
      "Training Loss 0.00800\n",
      "Training Loss 0.00799\n",
      "Training Loss 0.00798\n",
      "Training Loss 0.00797\n",
      "Training Loss 0.00796\n",
      "Training Loss 0.00795\n",
      "Training Loss 0.00794\n",
      "Training Loss 0.00793\n",
      "Training Loss 0.00792\n",
      "Training Loss 0.00791\n",
      "Training Loss 0.00790\n",
      "Training Loss 0.00789\n",
      "Training Loss 0.00788\n",
      "Training Loss 0.00787\n",
      "Training Loss 0.00787\n",
      "Training Loss 0.00786\n",
      "Training Loss 0.00785\n",
      "Training Loss 0.00784\n",
      "Training Loss 0.00783\n",
      "Training Loss 0.00782\n",
      "Training Loss 0.00781\n"
     ]
    }
   ],
   "source": [
    "Learn(net, train[0], train[1], epochs=500, lrate=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# net2.Learn(train[0], train[1], epochs=500, lrate=1.)\n",
    "# print('Training Set')\n",
    "# CE = net2.Evaluate(train[0], train[1])\n",
    "# accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "# print('Cross Entropy = '+str(CE))\n",
    "# print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "# print('Test Set')\n",
    "# CE = net2.Evaluate(test[0], test[1])\n",
    "# accuracy = net2.ClassificationAccuracy(test[0], test[1])\n",
    "# print('Cross Entropy = '+str(CE))\n",
    "# print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1D -> 1D (linear mapping)\n",
    "np.random.seed(846)\n",
    "n_input = 1\n",
    "n_output = 1\n",
    "slope = np.random.rand() - 0.5\n",
    "intercept = np.random.rand()*2. - 1.\n",
    "\n",
    "def myfunc(x):\n",
    "    return slope*x+intercept\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 200\n",
    "training_output = []\n",
    "training_input = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    training_input.append(np.array([x]))\n",
    "    training_output.append(np.array([t]))\n",
    "\n",
    "# Create a testing dataset\n",
    "n_samples = 50\n",
    "test_input = []\n",
    "test_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx] + np.random.normal(scale=0.1)\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    test_input.append(np.array([x]))\n",
    "    test_output.append(np.array([t]))\n",
    "\n",
    "# Create a perfect dataset\n",
    "n_samples = 100\n",
    "perfect_input = []\n",
    "perfect_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x)\n",
    "    perfect_input.append(np.array([x]))\n",
    "    perfect_output.append(np.array([t]))\n",
    "    \n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]\n",
    "perfect = [np.array(perfect_input), np.array(perfect_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1214,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = Network([1, 10, 1], type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.35753196157541495\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import Network_solutions as sol\n",
    "# net2 = sol.Network([1, 10, 1], type='regression')\n",
    "# import copy\n",
    "# net2.W = copy.deepcopy(net.W)\n",
    "# mse = net2.Evaluate(train[0], train[1])\n",
    "# print('MSE = '+str(mse))\n",
    "# net2.Learn(train[0], train[1], epochs=300)\n",
    "# mse = net2.Evaluate(train[0], train[1])\n",
    "# print('Training MSE = '+str(mse))\n",
    "# mse = net2.Evaluate(test[0], test[1])\n",
    "# print('Test MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE = 0.01394357278018016\n"
     ]
    }
   ],
   "source": [
    "# On training dataset\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('Training MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE = 0.017116776538103968\n"
     ]
    }
   ],
   "source": [
    "# On test dataset\n",
    "mse = net.Evaluate(test[0], test[1])\n",
    "print('Test MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1219,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model and the TRUE solution (since we know it)\n",
    "s = np.linspace(-1, 1, 200)\n",
    "y = net.FeedForward(np.array([s]).T)\n",
    "p = [myfunc(x) for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1220,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training data,\n",
    "# as well as out model and the true model\n",
    "plt.plot(s,y, 'r--')\n",
    "plt.plot(s,p, 'g:')\n",
    "plt.plot(training_input, training_output, 'bo')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
