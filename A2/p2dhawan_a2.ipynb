{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# p2dhawan_a2 (v1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import importlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q1: Logistic Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The logistic function may be written as:\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\sigma (z)}&= \\frac{1} {1 + e^{-z}} \\\\\n",
    "{\\sigma (z)}&= \\frac{e^{z}} {e^{z}+1}\n",
    "\\end{align}\n",
    "$$\n",
    "Now, using Quotient rule:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d \\sigma(z)}{dz} &= \\frac{e^{z}(e^{z}+1) - e^{z}.e^{z}}{(1+e^{z})^2} \\\\\n",
    "\\frac{d \\sigma(z)}{dz} &= \\frac{e^{z}}{(1+e^{z})^2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Now,\n",
    "$$\n",
    "\\begin{align}\n",
    "{\\sigma(z)(1-\\sigma(z))} &= \\frac{e^{z}}{1+e^{z}}.\\frac{1 + e^{z} - e^{z}}{1 + e^{z}} \\\\\n",
    "{\\sigma(z)(1-\\sigma(z))} &= \\frac{e^{z}}{(1+e^{z})^2} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore, LHS = RHS.\n",
    "Hence proved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q2: Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, lets find the gradient of the softmax function, using Quotient rule:   \n",
    "If m = k:  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y_{k}}{\\partial z_{m}} &= \\frac{({e^{z_{k}}.\\sum_{j=1}^{K}{e^{z_{j}}}}) - e^{z_{m}}.e^{z_{k}}}{(\\sum_{j=1}^{K}{e^{z_{j}}})^{2}} \\\\\n",
    "\\frac{\\partial y_{k}}{\\partial z_{m}} &= \\frac{e^{z_k}.(\\sum_{j=1}^{K}{e^{z_{j}} - e^{z_{m}}})}{(\\sum_{j=1}^{K}{e^{z_{j}}}).(\\sum_{j=1}^{K}{e^{z_{j}}})}\\\\\n",
    "\\frac{\\partial y_{k}}{\\partial z_{m}} &= y_{k}.(1 - y_{m}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "If m!=k, applying the quotient rule simplifies to:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial y_{k}}{\\partial z_{m}} &= \\frac{- e^{z_k}.e^{z_{m}}}{(\\sum_{j=1}^{K}{e^{z_{j}}})^{2}} \\\\\n",
    "\\frac{\\partial y_{k}}{\\partial z_{m}} &= - y_{k}y_{m} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Now, let's compute the derivative of the categorical cross entropy and apply chain rule. The important point is to gather all the yj's that contribute the zj:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_{j}} &= -\\sum_{k=1}^{K}{t_{k}}.\\frac{\\partial ln(y_{k})}{\\partial z_{j}}\\\\\n",
    "&= -\\sum_{k=1}^{K}{\\frac{t_{k}}{y_{k}}}.\\frac{\\partial y_{k}}{\\partial z_{j}}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Substituting the derivative of the Softmax function from above:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_{j}} &= (\\sum_{k \\neq j}^{K}{(y_{k}.y_{j})\\frac{t_{k}}{y_{k}}}) - t_{j}(1-y_{j})\\\\\n",
    "&= -t_{j} + y_{j}((\\sum_{k \\neq j}^{K}{t_{k}) + t_{j}})\n",
    "\\end{align}\n",
    "$$\n",
    "If we assume the given problem is multi-class single-label (a fair assumption to make since we are using softmax), then:  \n",
    "$$\n",
    "\\begin{align}\n",
    "(\\sum_{k \\neq j}^{K}{t_{k}) + t_{j}} &= 1\n",
    "\\end{align}\n",
    "$$\n",
    "Therefore our result is:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z_{j}} &= y_{j} - t_{j}\n",
    "\\end{align}\n",
    "$$\n",
    "Hence, proved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Q3: Top-Layer Error Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "a) Logistic activation function and binary cross entropy loss:  \n",
    "From q1, we know:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d \\sigma(z)}{dz} &= {\\sigma(z)(1-\\sigma(z))} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Let's compute the derivative for the binary cross entropy loss:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial y} &= \\frac{\\partial(-tlny - (1-t)ln(1-y)}{\\partial y} \\\\\n",
    "&=\\frac{-\\partial(t lny)}{\\partial y} - \\frac{\\partial((1-t)ln(1-y))}{\\partial y} \\\\\n",
    "&= \\frac{-t}{y} + \\frac{1-t}{1-y} \\\\\n",
    "&= \\frac{y-t}{y(1-y)}\n",
    "\\end{align}\n",
    "$$\n",
    "Using the chain rule and plugging in the partial derivative for logistic, as y is the logistic function:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z} &=\\frac{y-t}{y(1-y)}.y(1-y)\\\\\n",
    "&= y - t\n",
    "\\end{align}\n",
    "$$\n",
    "Hence, the result.\n",
    "  \n",
    "  \n",
    "b)MSE with Identity Activation Function\n",
    "We know, for the identity function:\n",
    "$$\n",
    "\\begin{align}\n",
    "y(z) = \\sigma{(z)} = z \\\\ \n",
    "\\frac{d y}{d z} = 1 \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "Computing the gradient for the MSE function:\n",
    "$$\n",
    "\\begin{align}\n",
    "E(y, t) &= (y-t)^2 \\\\\n",
    "\\frac{\\partial E}{\\partial y} &= \\frac{\\partial(y-t)^2}{\\partial y} \\\\\n",
    "&= 2(y-t)\n",
    "\\end{align}\n",
    "$$\n",
    "Now, applying chain rule:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial z} &= \\frac{\\partial E}{\\partial y}. \\frac{\\partial y}{\\partial z} \\\\\n",
    "&= 2(y-t)\n",
    "\\end{align}\n",
    "$$\n",
    "Hence, the result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4: Implementing Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Supplied Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     16,
     35
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Supplied functions\n",
    "\n",
    "def NSamples(x):\n",
    "    '''\n",
    "        n = NSamples(x)\n",
    "        \n",
    "        Returns the number of samples in a batch of inputs.\n",
    "        \n",
    "        Input:\n",
    "         x   is a 2D array\n",
    "        \n",
    "        Output:\n",
    "         n   is an integer\n",
    "    '''\n",
    "    return len(x)\n",
    "\n",
    "def Shuffle(inputs, targets):\n",
    "    '''\n",
    "        s_inputs, s_targets = Shuffle(inputs, targets)\n",
    "        \n",
    "        Randomly shuffles the dataset.\n",
    "        \n",
    "        Inputs:\n",
    "         inputs     array of inputs\n",
    "         targets    array of corresponding targets\n",
    "         \n",
    "        Outputs:\n",
    "         s_inputs   shuffled array of inputs\n",
    "         s_targets  corresponding shuffled array of targets\n",
    "    '''\n",
    "    data = list(zip(inputs,targets))\n",
    "    np.random.shuffle(data)\n",
    "    s_inputs, s_targets = zip(*data)\n",
    "    return np.array(s_inputs), np.array(s_targets)\n",
    "\n",
    "def Logistic(z):\n",
    "    '''\n",
    "        y = Logistic(z)\n",
    "\n",
    "        Applies the logistic function to each element in z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return 1. / (1 + np.exp(-z) )\n",
    "\n",
    "def Logistic_p(h):\n",
    "    '''\n",
    "        yp = Logistic_p(h)\n",
    "        \n",
    "        Returns the slope of the logistic function at z when h = Logistic(z).\n",
    "        Note the h is the input, NOT z.\n",
    "    '''\n",
    "    return h*(1.-h)\n",
    "\n",
    "def Identity(z):\n",
    "    '''\n",
    "        y = Identity(z)\n",
    "\n",
    "        Does nothing... simply returns z.\n",
    "\n",
    "        Input:\n",
    "         z    is a scalar, list or array\n",
    "\n",
    "        Output:\n",
    "         y    is the same shape as z\n",
    "    '''\n",
    "    return z\n",
    "\n",
    "def Identity_p(h):\n",
    "    '''\n",
    "        yp = Identity_p(h)\n",
    "        \n",
    "        Returns the slope of the identity function h.\n",
    "    '''\n",
    "    return np.ones_like(h)\n",
    "\n",
    "def OneHot(z):\n",
    "    '''\n",
    "        y = OneHot(z)\n",
    "\n",
    "        Applies the one-hot function to the vectors in z.\n",
    "        Example:\n",
    "          OneHot([[0.9, 0.1], [-0.5, 0.1]])\n",
    "          returns np.array([[1,0],[0,1]])\n",
    "\n",
    "        Input:\n",
    "         z    is a 2D array of samples\n",
    "\n",
    "        Output:\n",
    "         y    is an array the same shape as z\n",
    "    '''\n",
    "    y = []\n",
    "    # Locate the max of each row\n",
    "    for zz in z:\n",
    "        idx = np.argmax(zz)\n",
    "        b = np.zeros_like(zz)\n",
    "        b[idx] = 1.\n",
    "        y.append(b)\n",
    "    y = np.array(y)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \n",
    "    def __init__(self, n_nodes, act='logistic'):\n",
    "        '''\n",
    "            lyr = Layer(n_nodes, act='logistic')\n",
    "            \n",
    "            Creates a layer object.\n",
    "            \n",
    "            Inputs:\n",
    "             n_nodes  the number of nodes in the layer\n",
    "             act      specifies the activation function\n",
    "                      Use 'logistic' or 'identity'\n",
    "        '''\n",
    "        self.N = n_nodes  # number of nodes in this layer\n",
    "        self.h = []       # node activities\n",
    "        self.b = np.zeros(self.N)  # biases\n",
    "        \n",
    "        # Activation functions\n",
    "        self.sigma = Logistic\n",
    "        self.sigma_p = (lambda : Logistic_p(self.h))\n",
    "        if act=='identity':\n",
    "            self.sigma = Identity\n",
    "            self.sigma_p = (lambda : Identity_p(self.h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     2,
     65,
     81
    ]
   },
   "outputs": [],
   "source": [
    "class Network():\n",
    "\n",
    "    def __init__(self, sizes, type='classifier'):\n",
    "        '''\n",
    "            net = Network(sizes, type='classifier')\n",
    "\n",
    "            Creates a Network and saves it in the variable 'net'.\n",
    "\n",
    "            Inputs:\n",
    "              sizes is a list of integers specifying the number\n",
    "                  of nodes in each layer\n",
    "                  eg. [5, 20, 3] will create a 3-layer network\n",
    "                      with 5 input, 20 hidden, and 3 output nodes\n",
    "              type can be either 'classifier' or 'regression', and\n",
    "                  sets the activation function on the output layer,\n",
    "                  as well as the loss function.\n",
    "                  'classifier': logistic, cross entropy\n",
    "                  'regression': linear, mean squared error\n",
    "        '''\n",
    "        self.n_layers = len(sizes)\n",
    "        self.lyr = []    # a list of Layers\n",
    "        self.W = []      # Weight matrices, indexed by the layer below it\n",
    "        \n",
    "        # Two common types of networks\n",
    "        # The member variable self.Loss refers to one of the implemented\n",
    "        # loss functions: MSE, or CrossEntropy.\n",
    "        # Call it using self.Loss(t)\n",
    "        if type=='classifier':\n",
    "            self.classifier = True\n",
    "            self.Loss = self.CrossEntropy\n",
    "            activation = 'logistic'\n",
    "        else:\n",
    "            self.classifier = False\n",
    "            self.Loss = self.MSE\n",
    "            activation = 'identity'\n",
    "\n",
    "        # Create and add Layers (using logistic for hidden layers)\n",
    "        for n in sizes[:-1]:\n",
    "            self.lyr.append( Layer(n) )\n",
    "   \n",
    "        # For the top layer, we use the appropriate activtaion function\n",
    "        self.lyr.append( Layer(sizes[-1], act=activation) )\n",
    "    \n",
    "        # Randomly initialize weight matrices\n",
    "        for idx in range(self.n_layers-1):\n",
    "            m = self.lyr[idx].N\n",
    "            n = self.lyr[idx+1].N\n",
    "            temp = np.random.normal(size=[m,n])/np.sqrt(m)\n",
    "            self.W.append(temp)\n",
    "\n",
    "\n",
    "    def FeedForward(self, x):\n",
    "        '''\n",
    "            y = net.FeedForward(x)\n",
    "\n",
    "            Runs the network forward, starting with x as input.\n",
    "            Returns the activity of the output layer.\n",
    "        '''\n",
    "        x = np.array(x)  # Convert input to array, in case it's not\n",
    "        \n",
    "        # layer.h will be of the shape : [num_samples, num_nodes]\n",
    "        for i in range(len(self.lyr)):\n",
    "            if i == 0:\n",
    "                self.lyr[i].h = x\n",
    "            else:\n",
    "                last_layer_output = self.lyr[i-1].h\n",
    "                linear = np.dot(last_layer_output, self.W[i-1])\n",
    "                linear += np.outer(np.ones(len(x)), self.lyr[i].b)\n",
    "                self.lyr[i].h = self.lyr[i].sigma(linear)\n",
    "                \n",
    "        \n",
    "        return self.lyr[-1].h\n",
    "\n",
    "    \n",
    "    def Evaluate(self, inputs, targets):\n",
    "        '''\n",
    "            E = net.Evaluate(data)\n",
    "\n",
    "            Computes the average loss over the supplied dataset.\n",
    "\n",
    "            Inputs\n",
    "             inputs  is an array of inputs\n",
    "             targets is a list of corresponding targets\n",
    "\n",
    "            Outputs\n",
    "             E is a scalar, the average loss\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        return self.Loss(targets)\n",
    "\n",
    "    def ClassificationAccuracy(self, inputs, targets):\n",
    "        '''\n",
    "            a = net.ClassificationAccuracy(data)\n",
    "            \n",
    "            Returns the fraction (between 0 and 1) of correct one-hot classifications\n",
    "            in the dataset.\n",
    "        '''\n",
    "        y = self.FeedForward(inputs)\n",
    "        yb = OneHot(y)\n",
    "        n_incorrect = np.sum(yb!=targets) / 2.\n",
    "        return 1. - float(n_incorrect) / NSamples(inputs)\n",
    "\n",
    "    \n",
    "    def CrossEntropy(self, t):\n",
    "        '''\n",
    "            E = net.CrossEntropy(t)\n",
    "\n",
    "            Evaluates the mean cross entropy loss between t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        \n",
    "        output_activity = self.lyr[-1].h # shape = (num_samples, num_outputs)\n",
    "        #shape of t: (num_samples, num_outputs)\n",
    "        \n",
    "        # Equation for Cross Entropy (or negative log likelihood for observing the data given the parameters)\n",
    "        # - (ylog(y) + (1-y)log(1-p))\n",
    "        # Take the mean\n",
    "        result = 0\n",
    "        for i in range(len(t)):\n",
    "            for j in range(t.shape[1]):\n",
    "                if t[i][j] == 1:\n",
    "                        result += np.log(output_activity[i][j])\n",
    "                else:\n",
    "                        result += np.log(1.-output_activity[i][j])\n",
    "        return (-1*result)/len(t)\n",
    "\n",
    "    \n",
    "    def MSE(self, t):\n",
    "        '''\n",
    "            E = net.MSE(t)\n",
    "\n",
    "            Evaluates the MSE loss function using t and the activity of the top layer.\n",
    "            To evaluate the network's performance on an input/output pair (x,t), use\n",
    "              net.FeedForward(x)\n",
    "              E = net.Loss(t)\n",
    "\n",
    "            Inputs:\n",
    "              t is an array holding the target output\n",
    "\n",
    "            Outputs:\n",
    "              E is the loss function for the given case\n",
    "        '''\n",
    "        output_activity = self.lyr[-1].h\n",
    "        error = np.mean(np.square(output_activity - t))\n",
    "        \n",
    "        return error\n",
    "\n",
    "    \n",
    "    def BackProp(self, t, lrate=0.05):\n",
    "        '''\n",
    "            net.BackProp(targets, lrate=0.05)\n",
    "            \n",
    "            Given the current network state and targets t, updates the connection\n",
    "            weights and biases using the backpropagation algorithm.\n",
    "            \n",
    "            Inputs:\n",
    "             t      an array of targets (number of samples must match the\n",
    "                    network's output)\n",
    "             lrate  learning rate\n",
    "        '''\n",
    "        t = np.array(t)  # convert t to an array, in case it's not\n",
    "        \n",
    "        # We are following (i-1) -> i at index i\n",
    "        for i in range(len(self.lyr)-1,0,-1):\n",
    "            if i == len(self.lyr)-1:\n",
    "                de_dz = (self.lyr[-1].h - t).T # The gradient w.r.t. to the outermost layer: Y x P\n",
    "            else:\n",
    "                dh_dz = self.lyr[i].sigma_p().T # This is H x P\n",
    "                # W[i] is H x Y, de_dz is Y x P\n",
    "                de_dz = np.multiply(dh_dz, np.dot(self.W[i], de_dz)) # This will be H x P\n",
    "                \n",
    "            # lyr[i-1].h is P x H, de_dz is Y x P or H(i+1) x P\n",
    "            de_dw = np.dot(de_dz, self.lyr[i-1].h).T # To simplify : This will be H x Y\n",
    "            de_dw /= t.shape[0]\n",
    "            \n",
    "            de_db = np.sum(de_dz, axis=1)/t.shape[0] # Length : Y\n",
    "            self.lyr[i].b -= de_db\n",
    "            \n",
    "            self.W[i-1] -= lrate*de_dw\n",
    "\n",
    "        \n",
    "\n",
    "    def Learn(self, inputs, targets, lrate=0.05, epochs=1):\n",
    "        '''\n",
    "            Network.Learn(inputs, targets, lrate=0.05, epochs=1)\n",
    "\n",
    "            Run through the dataset 'epochs' number of times, incrementing the\n",
    "            network weights for each training sample. For each epoch, it\n",
    "            shuffles the order of the samples.\n",
    "\n",
    "            Inputs:\n",
    "              inputs  is an array of input samples\n",
    "              targets is a corresponding array of targets\n",
    "              lrate   is the learning rate (try 0.001 to 0.5)\n",
    "              epochs  is the number of times to go through the training data\n",
    "        '''\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            train_inputs, train_targets = Shuffle(inputs, targets)\n",
    "            y = self.FeedForward(train_inputs)\n",
    "            self.BackProp(train_targets, lrate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Classification Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 5 Classes in 8-Dimensional Space\n",
    "np.random.seed(15)\n",
    "noise = 0.1\n",
    "InputClasses = np.array([[1,0,1,0,0,1,1,0],\n",
    "                         [0,1,0,1,0,1,0,1],\n",
    "                         [0,1,1,0,1,0,0,1],\n",
    "                         [1,0,0,0,1,0,1,1],\n",
    "                         [1,0,0,1,0,1,0,1]], dtype=float)\n",
    "OutputClasses = np.array([[1,0,0,0,0],\n",
    "                          [0,1,0,0,0],\n",
    "                          [0,0,1,0,0],\n",
    "                          [0,0,0,1,0],\n",
    "                          [0,0,0,0,1]], dtype=float)\n",
    "n_input = np.shape(InputClasses)[1]\n",
    "n_output = np.shape(OutputClasses)[1]\n",
    "n_classes = np.shape(InputClasses)[0]\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 100\n",
    "training_output = []\n",
    "training_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    training_input.append(x)\n",
    "    training_output.append(t)\n",
    "\n",
    "# Create a test dataset\n",
    "n_samples = 100\n",
    "test_output = []\n",
    "test_input = []\n",
    "for idx in range(n_samples):\n",
    "    k = np.random.randint(n_classes)\n",
    "    x = InputClasses[k,:] + np.random.normal(size=n_input)*noise\n",
    "    t = OutputClasses[k,:]\n",
    "    test_input.append(x)\n",
    "    test_output.append(t)\n",
    "\n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Create a Network\n",
    "net = Network([n_input, 18, n_output], type='classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 3.617051325333447\n",
      "     Accuracy = 26.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=500, lrate=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "Cross Entropy = 0.017728922146593974\n",
      "     Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Training Set')\n",
    "CE = net.Evaluate(train[0], train[1])\n",
    "accuracy = net.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set\n",
      "Cross Entropy = 0.01903443433918009\n",
      "     Accuracy = 100.0%\n"
     ]
    }
   ],
   "source": [
    "print('Test Set')\n",
    "CE = net.Evaluate(test[0], test[1])\n",
    "accuracy = net.ClassificationAccuracy(test[0], test[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## You can also try using the solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Entropy = 0.01903443433918009\n",
      "     Accuracy = 0.0%\n"
     ]
    }
   ],
   "source": [
    "import Network_solutions as sol\n",
    "net2 = sol.Network([n_input, 18, n_output], type='classifier')\n",
    "accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set\n",
      "Cross Entropy = 0.8874576713656723\n",
      "     Accuracy = 96.0%\n",
      "Test Set\n",
      "Cross Entropy = 0.9454835621238553\n",
      "     Accuracy = 92.0%\n"
     ]
    }
   ],
   "source": [
    "net2.Learn(train[0], train[1], epochs=500, lrate=1.)\n",
    "print('Training Set')\n",
    "CE = net2.Evaluate(train[0], train[1])\n",
    "accuracy = net2.ClassificationAccuracy(train[0], train[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')\n",
    "print('Test Set')\n",
    "CE = net2.Evaluate(test[0], test[1])\n",
    "accuracy = net2.ClassificationAccuracy(test[0], test[1])\n",
    "print('Cross Entropy = '+str(CE))\n",
    "print('     Accuracy = '+str(accuracy*100.)+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Create a Regression Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 1D -> 1D (linear mapping)\n",
    "np.random.seed(846)\n",
    "n_input = 1\n",
    "n_output = 1\n",
    "slope = np.random.rand() - 0.5\n",
    "intercept = np.random.rand()*2. - 1.\n",
    "\n",
    "def myfunc(x):\n",
    "    return slope*x+intercept\n",
    "\n",
    "# Create a training dataset\n",
    "n_samples = 200\n",
    "training_output = []\n",
    "training_input = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    training_input.append(np.array([x]))\n",
    "    training_output.append(np.array([t]))\n",
    "\n",
    "# Create a testing dataset\n",
    "n_samples = 50\n",
    "test_input = []\n",
    "test_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx] + np.random.normal(scale=0.1)\n",
    "    t = myfunc(x) + np.random.normal(scale=0.1)\n",
    "    test_input.append(np.array([x]))\n",
    "    test_output.append(np.array([t]))\n",
    "\n",
    "# Create a perfect dataset\n",
    "n_samples = 100\n",
    "perfect_input = []\n",
    "perfect_output = []\n",
    "xv = np.linspace(-1, 1, n_samples)\n",
    "for idx in range(n_samples):\n",
    "    #x = np.random.rand()*2. - 1.\n",
    "    x = xv[idx]\n",
    "    t = myfunc(x)\n",
    "    perfect_input.append(np.array([x]))\n",
    "    perfect_output.append(np.array([t]))\n",
    "    \n",
    "train = [np.array(training_input), np.array(training_output)]\n",
    "test = [np.array(test_input), np.array(test_output)]\n",
    "perfect = [np.array(perfect_input), np.array(perfect_output)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "net = Network([1, 10, 1], type='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 0.35753196157541495\n"
     ]
    }
   ],
   "source": [
    "# Evaluate it before training\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.Learn(train[0], train[1], epochs=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Evaluate it After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE = 0.01394357278018016\n"
     ]
    }
   ],
   "source": [
    "# On training dataset\n",
    "mse = net.Evaluate(train[0], train[1])\n",
    "print('Training MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE = 0.017116776538103968\n"
     ]
    }
   ],
   "source": [
    "# On test dataset\n",
    "mse = net.Evaluate(test[0], test[1])\n",
    "print('Test MSE = '+str(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Evaluate our model and the TRUE solution (since we know it)\n",
    "s = np.linspace(-1, 1, 200)\n",
    "y = net.FeedForward(np.array([s]).T)\n",
    "p = [myfunc(x) for x in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training data,\n",
    "# as well as out model and the true model\n",
    "plt.plot(s,y, 'r--')\n",
    "plt.plot(s,p, 'g:')\n",
    "plt.plot(training_input, training_output, 'bo')\n",
    "plt.xlabel('Input')\n",
    "plt.ylabel('Output');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
